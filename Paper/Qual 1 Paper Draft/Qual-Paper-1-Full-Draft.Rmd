---
title             : "Two-Stage Path Analysis with Interaction: An Alternative Method of Modeling Latent Interaction Effects"
shorttitle        : "2S-PA-Int"

author: 
  - name          : "Gengrui (Jimmy) Zhang"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    email         : "gengruiz@email.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"

affiliation:
  - id            : "1"
    institution   : "University of Southhern California"

abstract: |
  Modeling interaction effects within the latent variable modeling framework has become increasingly popular in psychological research as it facilitates the exploration of in-depth theory and complex data structure. Comprared to the extensitvely used regression-based approaches assuming error-free variables, the latent interaction approach is able to account for measurement error and produce estimates with less bias and more accurate standard errors. In this study, we investigated three product indicator methods based on structural equation modeling (SEM): Matched-pair Unconstrained Product Indicator (UPI), Reliability-Adjusted Product Indicator (RAPI), and an extended model based on the two-stage path analysis (2S-PA) framework, namely 2S-PA-Int, by conducting a simulation study. The results showed that 2S-PA-Int produced consistently less standardized bias, acceptable relative SE bias and coverage rates, and lower RMSE values than matched-pair UPI and RAPI, particularly under the conditions of small sample size and low reliability. Generally, 2S-PA-Int showed promising statistical properties and simpler model specification, indicating that it could serve as a competitive alternative of existing methods. Future research directions of 2S-PA-Int were discussed.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "Latent interaction, UPI, RAPI, 2S-PA"

bibliography      : "r-references.bib"
notice            : "@kleinMaximumLikelihoodEstimation2000a"

floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(haven)
library(dplyr)
library(tidyr)
library(stringr)
library(here)
library(tidyverse)
library(semTools)
library(magick)
library(SimDesign)
r_refs("r-references.bib")

r_scripts <- list.files(here("R"), pattern = "\\.R$", full.names = TRUE)
lapply(r_scripts, source)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r message=FALSE, warning=FALSE}
sim <- readRDS(here("Sim_Data", "continuous_bound_09252024.rds"))
sim_results <- sim %>% 
  dplyr::select(-c(REPLICATIONS:ERRORS)) %>%
  mutate(across(where(is.numeric), round, 4),
         cor_xm = ifelse(cor_xm == 0, "0", cor_xm))
sim_null <- sim_results %>% filter(gamma_xm == 0)
sim_alt <- sim_results %>% filter(gamma_xm != 0)
```

Social science research increasingly focuses on intricate dynamics of complex phenomena, such as nonlinear and moderation effects, rather than merely simple bivariate relationship. This shift reflects the multifaceted nature of the real world, which seldom conforms to straightforward patterns [@cartePursuitModerationNine2003b; @mackinnonHowWhomMediation2008a; @cunninghamModerationSportManagement2019a]. For instance, while earlier studies have established that exercise contributes to weight loss, there is a burgeoning interest in understanding and probing into the underlying mechanisms, such as optimal timing, specific target populations, and the contextual conditions that modulate the effectiveness of exercise in promoting weight loss. Investigations into moderation, or interaction effects, provide critical insights into these inquiries by examining how additional variables—or an ensemble of variables—shape the dynamics between the primary variables of interest.

A prevalent approach to modeling moderation is through regression analysis, specifically by incorporating an interaction term, $XZ$:
\begin{equation}
Y = b_{0} + b_{1}X + b_{2}Z + b_{3}XZ + \epsilon,
\end{equation}
where $b_{0}$ is the intercept, $b_{1}$ and $b_{2}$ are the regression coefficients for $X$ and $Z$ respectively, $b_{3}$ is the coefficient for the interaction term $XZ$, and $\epsilon$ is the error term. To maintain consistency with the naming convention used by @marshStructuralEquationModels2004a, we refer to main effects (i.e., non-interaction effects) as "first-order effects". Hence $X$ and $Z$ are first-order variables and $b_{1}$ and $b_{2}$ are first-order effects in this case. 

Classical regression model typically assumes that variables are measured without error, a premise that can lead to biased parameter estimates when measurement errors are present in empirical research [@bollenStructuralEquationsLatent1989d; @cohenAppliedMultipleRegression2003; @carrollMeasurementErrorNonlinear2006]. This bias may be particularly more inflated for interaction effects [@andersonComparisonBiasMean1996]. To mitigate this issue, researchers use latent variables that are inferred and measured by a set of observed indicators within the structural equation modeling (SEM) framework, which can control and accommodate measurement errors in observed indicators [@bollenLatentVariablesPsychology2002a]. For example, depression is widely measured and assessed using the Center for Epidemiologic Studies Depression (CES-D) scale consisting of 20 items [@radloffCESDScaleSelfreport1977b]. An expanding body of research has demonstrated that SEM-based moderation models reliably provide more accurate representations of the relationships among latent constructs [@muellerStructuralEquationModeling1997a; @steinmetzThreeApproachesEstimate2011a; @chamEstimatingLatentVariable2012a; @maslowskyEstimatingInterpretingLatent2015a]. 

The two-stage path analysis (2S-PA; Lai & Hsiao, 2022) method models paths or pathway among latent variables through the use of factor scores. Simulation studies have shown its ability to yield parameter estimates with reduced standard error bias, enhanced convergence rates, and improved management of Type I error, particularly in small sample contexts [@laiTwostagePathAnalysis2022a; @laiCorrectingUnreliabilityPartial2023]. Given its promising statistical property, simpler model specification, and easier implementation in widely used software, we extended the 2S-PA method to incorporate latent interaction estimation in this study, and named it 2S-PA-Int. We reviewed two widely used latent interaction models using the product indicator method: Unconstrained Product Indicator with Matched Pairs (Matched-Pair UPI; Marsh et al., 2004) and Reliability-Adjusted Product Indicator (RAPI; Hsiao et al., 2018). Then we conducted a Monte Carlo simulation study to compare their performance with 2S-PA-Int. To proceed, we first introduced a classical model of latent interaction, and then presented UPI, RAPI, and 2S-PA-Int with technical details.

## A Classical Model of Latent Interaction

@kennyEstimatingNonlinearInteractive1984a introduced a seminal structural model for estimating latent interaction effects, particularly in scenarios involving two latent predictors and their interaction term:
\begin{equation}
y = \alpha + \gamma_{x}\xi_{x} + \gamma_{m}\xi_{m} + \gamma_{xm}\xi_{x}\xi_{m} + \zeta,
\end{equation} 
where $\alpha$ is the constant intercept, $\xi_{x}$ and $\xi_{m}$ are the first-order latent predictors, and the product $\xi_{x}\xi_{m}$ defines the interaction effect. Note that $\xi_{x}$ and $\xi_{m}$ are allowed to correlate with each other. The disturbance term $\zeta$ in the model is assumed to follow a normal distribution, $\zeta \sim N(0, \psi)$, where $\psi$ denotes the variance of $\zeta$, accounting for unobserved factors that influence the dependent variable.  The coefficients $\gamma_{x}$ and $\gamma_{m}$ capture the first-order effects of the latent predictors, while $\gamma_{xm}$ measures the latent interaction effect. The dependent variable $y$ in this model can be either an observed variable or a latent construct, allowing for flexibility in its application.

The measurement model for the first-order latent predictors, such as $\xi_{x}$, can be articulated by the following confirmatory factor analysis (CFA) framework:
\begin{equation}
\mathbf{x} = \boldsymbol{\tau_{x}} + \boldsymbol{\lambda_{x}}\xi_{x} + \boldsymbol{\delta_{x}},
\end{equation}
wherein, for each indicator $i = 1, \ 2, \ ..., \ p_{x}$ associated with the latent predictor $\xi_{x}$, $\mathbf{x}$ denotes a $p_{x} \times 1$ vector of observed first-order indicators (i.e., the indicators of $\xi_{x}$). The term $\boldsymbol{\tau_{x}}$ represents a $p_{x} \times 1$ vector of constant intercepts, while $\boldsymbol{\lambda}{x}$ is a $p_{x} \times 1$ vector of factor loadings, which capture the strength of the relationship between the latent variable $\xi{x}$ and each of its indicators. The vector $\boldsymbol{\delta_{x}}$ represents the $p_{x} \times 1$ vector of measurement errors associated with these indicators. Each measurement error $\delta_{x_{i}}$ is normally distributed with a mean of zero and a variance of $\theta_{x_{i}}$. Under the assumption of local independence, which posits that the first-order indicators are uncorrelated with one another when they are indicators of the same latent variable, the variance-covariance matrix of all the indicators' measurement errors is a diagonal matrix, denoted as $\mathbf{\Theta_{\delta_{x}}} = \text{diag}(\theta_{x_{1}}, \theta_{x_{2}}, ..., \theta_{x_{p}})$. This measurement model, along with its associated parameters, is similarly applicable to the latent predictor $\xi_{m}$, ensuring consistency in the modeling of both latent variables.

Kenny and Judd's original formulation of model omitted the intercept $\alpha$, a point subsequently addressed by @Jreskog1996NonlinearSE, who revised the model under a set of assumptions. The revised latent interaction model is grounded in three primary assumptions related to multivariate normal distribution and independence: (1) The measurement errors of first-order indicators, the first-order latent predictors, and the disturbance term in the structural model are multivariate normal, uncorrelated, and independent to each other (i.e., $Corr[\delta, \xi] = 0$; $Corr[\zeta, \xi] = 0$; $Corr[\delta, \zeta] = 0$, where $Corr$ denotes the correlation index); (2) All measurement errors are mutually independent and uncorrelated to each other (i.e., $Corr[\delta_{i}, \delta_{i'}] = 0$ for $i \neq i'$); (3) The correlation between the first-order latent predictors, $Corr[\xi_{x}, \xi_{m}]$, is assumed to be non-zero and is freely estimated. This approach accounts for the fact that the product term $\xi_{x}\xi_{m}$ may exhibit a non-normal distribution even when $\xi_{x}$ and $\xi_{m}$ are themselves normally distributed with means of 0 [@Jreskog1996NonlinearSE].

@alginaNoteEstimatingJoreskogYang2001 refined Jöreskog and Yang's (1996) model by introducing the use of mean-centered first-order indicators (e.g., $x_{i} - \overline{x_{i}}$, where $\overline{x_{i}}$ represents the mean of $x_{i}$) to construct product indicators (PI) that capture the latent interaction term. This enhancement significantly improves the model by rendering parameter estimates more interpretable, facilitating a higher rate of model convergence, and reducing estimation bias [@alginaNoteEstimatingJoreskogYang2001; @moulderComparisonMethodsEstimating2002a; @marshStructuralEquationModels2004a]. Moreover, the practice of mean-centering first-order indicators effectively mitigates the problem of multicollinearity, thereby more distinctly delineating the contributions of the first-order latent variables and their interactions, as highlighted by @schoemannTestingInterpretingLatent2021.

## Unconstrained Product Indicator (UPI)

While @alginaNoteEstimatingJoreskogYang2001 significantly improved the model, their approach required complicated nonlinear constraints on parameters of PIs and the interaction term. Constraints in SEM are predefined conditions or restrictions applied to model parameters to ensure model identifiability, theoretical consistency, and interpretability [@klinePrinciplesPracticeStructural2016]. Consider, for example, that $x_{2}$ and $m_{2}$ are two first-order indicators of respective latent predictors $\xi_{x}$ and $\xi_{m}$, with their corresponding PI formed as $x_{2}m_{2}$. Then $x_{2}m_{2}$ can be decomposed using the measurement model of $x_{2}$ and $m_{2}$:

\begin{equation}
x_{2}m_{2}= (\lambda_{x_{2}}\xi_{x} + \delta_{x_{2}})(\lambda_{m_{2}}\xi_{m} + \delta_{m_{2}}),
\end{equation}
where $\lambda$ is the factor loading, $\xi$ is the first-order latent variable, and $\delta$ is the error term of first-order indicators. After expanding the equation, it can be shown that the factor loading of this formed PI is a function of first-order indicators' factor loadings, such that $\lambda_{x_{2}m_{2}} = \lambda_{x_{2}}\lambda_{m_{2}}$. Similarly, the error term can be derived as a function of parameters from first-order indicators: $\delta_{x_{2}m_{2}} = \lambda_{x_{2}}\xi_{x}\delta_{m_{2}} + \lambda_{m_{2}}\xi_{m}\delta_{x_{2}} + \delta_{x_{2}}\delta_{m_{2}}$. As the number of first-order indicators increases, the model specification becomes overwhelmingly cumbersome due to the resultant nonlinear constraints, which can pose challenges to model convergence.

@marshStructuralEquationModels2004a explored methods to eliminate these complex constraints and introduced the innovative Unconstrained Product Indicator (UPI) approach, which simplifies model specification and decreases the likelihood of convergence issues. The structural model of UPI is identical to the model presented in equation (2), with the exception of omitting the intercept $\alpha$. To illustrate this approach, consider a measurement model where the latent variables $\xi_{x}$ and $\xi_{m}$ are each associated with three indicators:

\begin{align}
    \begin{bmatrix}
        x_{1} \\
        x_{2} \\ 
        x_{3}
    \end{bmatrix} =
    \begin{bmatrix}
        \tau_{x_{1}} \\
        \tau_{x_{2}} \\ 
        \tau_{x_{3}}
    \end{bmatrix} +
    \begin{bmatrix}
        \lambda_{x_{1}} \\
        \lambda_{x_{2}} \\ 
        \lambda_{x_{3}}
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{x} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{x_{1}} \\
        \delta_{x_{2}} \\ 
        \delta_{x_{3}}
    \end{bmatrix},
\end{align}

\begin{align}
    \begin{bmatrix}
        m_{1} \\
        m_{2} \\ 
        m_{3}
    \end{bmatrix} =
    \begin{bmatrix}
        \tau_{m_{1}} \\
        \tau_{m_{2}} \\ 
        \tau_{m_{3}}
    \end{bmatrix} +
    \begin{bmatrix}
        \lambda_{m_{1}} \\
        \lambda_{m_{2}} \\ 
        \lambda_{m_{3}}
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{m_{1}} \\
        \delta_{m_{2}} \\ 
        \delta_{m_{3}}
    \end{bmatrix}
\end{align} 

@marshStructuralEquationModels2004a introduced two methods for specifying UPI: the all-pair UPI and the matched-pair UPI. In the all-pair UPI model, the latent interaction term is represented by all possible pairings of the first-order indicators of $\xi_{x}$ and $\xi_{m}$:

\begin{align}
    \begin{bmatrix}
        x_{1}m_{1} \\
        x_{1}m_{2} \\
        x_{1}m_{3} \\ 
        x_{2}m_{1} \\
        ... \\
        x_{3}m_{3}
    \end{bmatrix} = 
    \begin{bmatrix}
        \tau_{x_{1}m_{1}} \\
        \tau_{x_{1}m_{2}} \\ 
        \tau_{x_{1}m_{3}} \\ 
        \tau_{x_{2}m_{1}} \\ 
        ...\\
        \tau_{x_{3}m_{3}} 
    \end{bmatrix} +
    \begin{bmatrix}
        \lambda_{x_{1}m_{1}} \\
        \lambda_{x_{1}m_{2}} \\ 
        \lambda_{x_{1}m_{3}} \\ 
        \lambda_{x_{2}m_{1}} \\ 
        ...\\
        \lambda_{x_{3}m_{3}}
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{x}\xi_{m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{x_{1}m_{1}} \\
        \delta_{x_{1}m_{2}} \\ 
        \delta_{x_{1}m_{3}} \\
        \delta_{x_{2}m_{1}} \\
        ... \\
        \delta_{x_{3}m_{3}}
    \end{bmatrix},
\end{align}
where each PI is derived from multiplying two corresponding mean-centered first-order indicators, one from $\xi_{x}$ and the other from $\xi_{m}$ (e.g., the PI $x_{1}m_{1}$ is formed by the product of $x_{1}$ and $m_{1}$). The coefficients ${\tau_{x_{i}m_{i}}}$, ${\lambda_{x_{i}m_{i}}}$ and ${\delta_{x_{i}m_{i}}}$ are freely estimated as intercepts, factor loadings and measurement errors, respectively. The total number of PI is the multiplicative product of the number of first-order indicators for each latent predictor. In this case, nine unique PIs are formed ($3 \times 3 = 9$). 

Regarding the matched-pair UPI, the indicators are matched to create PIs:

\begin{align}
    \begin{bmatrix}
        x_{1}m_{1} \\
        x_{2}m_{2} \\
        x_{3}m_{3}
    \end{bmatrix} =
    \begin{bmatrix}
        \tau_{x_{1}m_{1}} \\
        \tau_{x_{2}m_{2}} \\ 
        \tau_{x_{3}m_{3}}
    \end{bmatrix} + 
    \begin{bmatrix}
        \lambda_{x_{1}m_{1}} \\
        \lambda_{x_{2}m_{2}} \\ 
        \lambda_{x_{3}m_{3}} 
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{x}\xi_{m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{x_{1}m_{1}} \\
        \delta_{x_{2}m_{2}} \\ 
        \delta_{x_{3}m_{3}}
    \end{bmatrix}
\end{align}

This alternative formulation leads to a significantly reduced number of PIs due to its simplicity. @marshStructuralEquationModels2004a argued that the matched-pair UPI is preferable based on two key criteria: (1) It leverages all available information by utilizing every first-order indicator, and (2) It avoids redundancy by ensuring that no first-order indicator is used more than once. Consequently, the matched-pair UPI method is recommended for its simplicity and effectiveness. Moreover, @marshStructuralEquationModels2004a demonstrated that the matched-pair UPI approach performs comparably to the all-pair model, exhibiting low bias and robustness to non-normal data. However, the matched-pair model is generally favored due to its greater simplicity and efficiency.

Since the mean of $\xi_{x}\xi_{m}$ may not equal to 0 even though $\xi_{x}$ and $\xi_{m}$ are assumed to have 0 means, @marshStructuralEquationModels2004a included a mean structure in their UPI model: $\mathbf{\kappa} = (0,\ 0,\ Cov[\xi_{x}, \xi_{m}])^T$, where $\mathbf{\kappa}$ should be the means of the three latent variables (see Algina & Boulder [2001] for more details). This adjustment ensures that the model accurately reflects the statistical relations between the first-order latent variables and their interaction term. @linStructuralEquationModels2010b further simplified the model by proposing a Double Mean Centering (DMC) strategy, wherein PIs composed of paired mean-centered first-order indicators are mean-centered again (e.g., $x_{i}m_{i} - \overline{x_{i}m_{i}}$). DMC eliminates the need for including a mean structure in the UPI model and has been shown to perform well in parameter estimation, even when the normality assumption is violated. Consequently, we employed the UPI method with DMC in this study.

Although UPI with DMC has simpler model specification and better performance of parameter estimation compared to the classical model, an arbitrariness-complexity dilemma between the all-pair and the matched-pair methods remains unresolved [@foldnesChoiceProductIndicators2014]. Consider a scenario involving two complex psychological constructs as latent predictors, each requiring more than 10 indicators to adequately capture the theoretical constructs. The all-pair UPI method could result in a latent interaction term indicated by hundreds of PIs. While having a large number of items can enhance the representation of latent constructs and theoretically increase the statistical power for detecting subtle effects, it also tends to create a cumbersome model. This complexity can negatively affect interpretability, escalate computational demands, and lead to overfitting. On the other hand, the matched-pair UPI strategy simplifies the model by reducing the number of necessary PIs but introduces the challenge of PI selection, particularly when researchers must handle unbalanced numbers of first-order indicators. For unbalanced indicators, researchers must decide how to properly form PIs, as multiple solutions exist. They might aggregate several observed indicators into fewer parcels [@jackmanEstimatingLatentVariable2011a] or prioritize items with higher reliability for PI formation [@wuComparisonStrategiesForming2013]. However, there is no consensus on the optimal strategy for forming matched pairs. The considerable arbitrariness across different approaches introduces uncertainty in selecting the best strategy and complicates the decision-making process in model specification. To address this issue, @wuComparisonStrategiesForming2013 investigated two solutions in which researchers could form PIs by using highly reliable first-order indicators (i.e., items with higher factor loadings) while ignoring those with low reliability, or by matching parcels of the larger group of first-order indicators with indicators of the smaller group. They recommended to form PIs in accordance with the order of item reliability, emphasizing the importance of leveraging the most reliable indicators to enhance model performance.

## Reliability Adjusted Product Indicator (RAPI)

The RAPI method, introduced by @hsiaoEvaluationTwoMethods2018a, also involves forming PIs, but it does so by using composite scores (either sum or mean scores) of multiple observed items. Specifically, this approach aggregates all first-order indicators into single indicators (SIs) to indicate first-order latent variables, and multiplies the first-order PIs to form the SI to indicate the latent interaction term. Consequently, the resulting PI is itself an SI. This method effectively circumvents the issue of arbitrariness in indicator selection while using all information without redundancy. RAPI adjusts for measurement error in composite scores by constraining error variances of SIs, thereby ensuring that parameter estimates are less biased. The model can be succinctly represented as follows:
\begin{align}
    \begin{bmatrix}
        x_{comp} \\
        m_{comp} \\
        x_{comp} \cdot m_{comp}
    \end{bmatrix} = 
    \begin{bmatrix}
        \tau_{x_{comp}} \\
        \tau_{m_{comp}} \\ 
        \tau_{x_{comp} \cdot m_{comp}} 
    \end{bmatrix} + 
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\ 
        0 & 0 & 1 
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{x} \\  
        \xi_{m} \\ 
        \xi_{x}\xi_{m}
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{x_{comp}} \\
        \delta_{m_{comp}} \\ 
        \delta_{x_{comp} \cdot m_{comp}}
    \end{bmatrix},
\end{align}
where $x_{comp}$ and $m_{comp}$ are the composite scores formed by their corresponding first-order indicators, and $x_{comp} \cdot m_{comp}$ is the formed PI indicating the latent interaction term. These composite scores serve as SIs for their respective latent variables, with factor loadings uniformly constrained to $1$ for model identification. The measurement errors are represented by $\mathbf{\delta}$s.

A key characteristic of the RAPI method is its ability to accommodate measurement error in first-order indicators through the incorporation of error-variance constraints, which are calculated based on composite reliability. While composite reliability estimates for these error-variance constraints can be obtained using various methods, @hsiaoEvaluationTwoMethods2018a summarized and compared four normally used estimators for composite reliability: Cronbach's $\alpha$ [@cronbachCoefficientAlphaInternal1951], $\omega$ [@mcdonaldTheoreticalFoundationsPrincipal1970; @raykovEstimationCompositeReliability1997], the greatest lower bound reliability [@tenbergeGreatestLowerBound2004], and Coefficient H [@hancockReliabilityAradoxAssessing2011]. Suppose that $\rho_{xx'}$ denotes the reliability index, the error variance of composite scores can be shown as a function of the reliability index: 
\begin{equation}
\hat{\sigma}^2_{\delta_{x}} = (1 - \rho_{xx'})\hat{\sigma}^2_{{x}},
\end{equation}
where $\hat{\sigma}^2_{\delta_{x}}$ represents the estimated error variance and $\hat{\sigma}^2_{{x}}$ represents the estimated total variance of the indicator. Given that $\hat{\sigma}^2_{{x}} = {\hat{\sigma}^2_{\xi_{x}} + \hat{\sigma}^2_{\delta_{x}}}$ where $\hat{\sigma}^2_{\xi_{x}}$ represents the estimated latent variance of $\xi_{x}$, one can rearrange equation (10) to get $\hat{\sigma}_{\delta_{x}}^2 = [(1 - \rho_{xx'})/{\rho_{xx'}}]\hat{\sigma}^2_{\xi_{x}}$, as derived from classical test theory [@lordStatisticalTheoriesMental1968]. Thus, under the assumption of independently and identically distributed measurement error, the error-variance constraint of the interaction term $\xi_{x}\xi_{m}$ is:
\begin{equation}
\begin{aligned}
\hat{\sigma}^2_{\delta_{xm}} =\; & \rho_{xx'}\hat{\sigma}^2_{{x}}(1 - \rho_{mm'}\hat{\sigma}^2_{{m}})\; + \\&
                        \rho_{mm'}\hat{\sigma}^2_{{m}}(1-\rho_{xx'})\hat{\sigma}^2_{{x}}\; + \\&
                        (1 - \rho_{xx'})\hat{\sigma}^2_{{x}}(1 - \rho_{mm'})\hat{\sigma}^2_{{m}}. 
\end{aligned}
\end{equation}
More technical details are available in Appendix A of @hsiaoEvaluationTwoMethods2018a.

The use of composite scores as SIs evidently simplifies model specification, as the total number of PIs directly corresponds to the number of interaction terms. By accounting for measurement error, RAPI is expected to yield less biased estimates of interaction effects and exhibit enhanced statistical power. However, the method's effectiveness is contingent upon accurate estimation of reliability measures. Inaccurate reliability estimates, which form the basis for error constraints, can result in biased outcomes. Despite its manageable model complexity and ease of implementation, @hsiaoModelingMeasurementErrors2021 demonstrated that RAPI may produce non-positive definite matrices due to negative error variances and inflated interaction effect estimates, under conditions of low reliability (e.g., r = .70) and small sample size (e.g., N = 100). This suggests that RAPI may generate unstable interaction estimates under such conditions, highlighting the importance of carefully considering reliability and sample size when applying this method.

## Two-stage Path Analysis with Interaction (2S-PA-Int)

The 2S-PA method, as proposed by @laiTwostagePathAnalysis2022a, is an alternative approach to addressing measurement error within the context of multiple congeneric items (i.e., items with unique factor loadings and error variances; Jöreskog, 1971) by incorporating reliability adjustment. While it shares similarities with the RAPI method, 2S-PA uses factor scores as single indicators (SIs) for latent predictors. A key advancement of the 2S-PA approach is its capacity to assign observation-specific estimated reliability, thereby extending its applicability to ordered categorical items and accommodating distributions that deviate from normality [@laiTwostagePathAnalysis2022a; @laiCorrectingUnreliabilityPartial2023]. Moreover, conventional SEM models typically estimate measurement and structural models simultaneously, which necessitates an adequate sample size to achieve satisfactory convergence rates [@klinePrinciplesPracticeStructural2016; @kyriazosAppliedPsychometricsSample2018]. To address this potential issue, 2S-PA separates the step of specifying the measurement model from estimating the structural model, therefore alleviating computational burden and improving stability of parameter estimation. 

At the first stage of 2S-PA, researchers obtain factor scores using first-order indicators for each participant $j$ for $j = 1, 2, ..., n$. Next, parallel to RAPI, the factor scores of latent predictors are multiplied to construct a PI for the interaction term $\xi_{x_{j}}\xi_{m_{j}}$:

\begin{align}
    \begin{bmatrix}
        \tilde{x}_{j} \\ 
        \tilde{m}_{j} \\
        \widetilde{xm}_{j} 
    \end{bmatrix} = 
    \begin{bmatrix}
        \tau_{\tilde{x}_{j}} \\
        \tau_{\tilde{m}_{j}} \\ 
        \tau_{\widetilde{xm}_{j}}
    \end{bmatrix} + 
    \begin{bmatrix}
        \lambda_{\tilde{x}_{j}} & 0 & 0 \\
        0 & \lambda_{\tilde{m}_{j}} & 0 \\ 
        0 & 0 & \lambda_{\widetilde{xm}_{j}} 
    \end{bmatrix} 
    \begin{bmatrix}
        \xi_{x_{j}} \\  
        \xi_{m_{j}} \\
        \xi_{x_{j}}\xi_{m_{j}}
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{\tilde{x}_{j}} \\
        \delta_{\tilde{m}_{j}} \\ 
        \delta_{\widetilde{xm}_{j}}
    \end{bmatrix},
\end{align}
wherein the factor scores $\tilde{x}_{j}$, $\tilde{m}_{j}$ and the PI $\widetilde{xm}_{j}$ are SIs of the respective latent variables. The intercepts, factor loadings, and error variances are all model parameters to be freely estimated. 

Researchers have several methods available for calculating factor scores (e.g., regression factor scores, expected-a-posterior factor scores), as reviewed in @estabrookComparisonFactorScore2013. In this study, We used Bartlett factor scores that are adjusted to have the same units as latent variables and constrained their factor loadings to 1 (i.e., $\lambda_{\tilde{x}_{j}} = \lambda_{\tilde{m}_{j}} = \lambda_{\widetilde{xm}_{j}} = 1$), as shown in @devliegerHypothesisTestingUsing2016 and @laiCorrectingUnreliabilityPartial2023.

Given that the focus of the current study is on continuous variables, we assume that first-order indicators of $\xi_{x_{j}}$ and $\xi_{m_{j}}$ are normally distributed, and the corresponding error variances are constant across all observations. The error variance constraints for factor scores are $\hat{\sigma}_{\tilde{x}_{j}}^2$, where $\hat{\sigma}_{\tilde{x}_{j}}$ is the estimated standard error of measurement of the Bartlett factor score $\tilde{x}$ for the person $j$. The error-variance constraint for the interaction term is defined similarly as equation (11). In essence, the RAPI method is a special case of 2SPA where the composite scores are used in place of the factor scores [@laiTwostagePathAnalysis2022a].

In this paper, we investigated whether the 2S-PA-Int approach is a reliable alternative to existing methods for estimating latent interaction effects, for its simplicity in model complexity and clarity in model specification. @laiTwostagePathAnalysis2022a demonstrated that 2S-PA provides robust and precise estimates with less SE bias, lower Type I error rate, and higher convergence rates in conditions of small sample size and low reliability. Therefore, we aimed to examine whether the 2S-PA-Int method retains these advantages and delivers comparable performance in the estimation of latent interaction effects.

# Method

## Simulation Design

Adapted from @hsiaoModelingMeasurementErrors2021, the current simulation study aimed to systematically compare performance of moderated multiple regression (MMR), matched-pair UPI, RAPI, and 2S-PA-Int in estimating latent interaction effects for continuous congeneric items. We examined bias and variance of interaction estimates across various levels of sample size, reliability, and correlation between first-order latent variables. 

The population data was generated based on the model below with predefined parameter values:
\begin{equation}
\begin{gathered}
x_{i} =  \tau_{x_{i}} + \lambda_{x_{i}}\xi_{x} + \delta_{x_{i}};\\
m_{i} =  \tau_{m_{i}} + \lambda_{m_{i}}\xi_{m} + \delta_{m_{i}};\\
y =  \tau_{y} + \gamma_{x}\xi_{x} + \gamma_{m}\xi_{m} + \gamma_{xm}\xi_{x}\xi_{m} + \zeta,
\end{gathered}
\end{equation}
where the path coefficients of first-order latent predictors (i.e., $\gamma_{x}$ and $\gamma_{m}$) were both set to 0.3. The latent interaction term (i.e., $\gamma_{xm}$) was set to 0 for the zero effect condition and 0.3 for the non-zero effect condition. $\xi_{x}$ and $\xi_{m}$ were simulated from standard normal distribution, each indicated by three items (i.e., $\xi_{x}$ indicated by [$x_{1}$, $x_{2}$, $x_{3}$]; $\xi_{m}$ indicated by [$m_{1}$, $m_{2}$, $m_{3}$]). All first-order indicators and the dependent variable $y$ were observed continuous variables with normally distributed errors. Consequently, $\delta_{x_{i}}$, $\delta_{m_{i}}$ and $\zeta$ were assumed to follow a multivariate normal distribution and were mutually independent. The intercepts $\tau_{x_{i}}$, $\tau_{m_{i}}$, and $\tau_{y}$ were set to 0. Additionally, the first-order indicators were mean-centered for all the methods. 

Drawing from @joreskogStatisticalAnalysisSets1971, congeneric tests are defined as a set of observed items that measure a latent construct, each with different factor loadings and unique error variances. The error terms are assumed to be uncorrelated with each other and with the latent construct, thus representing random measurement error specific to each item. Following this concept, we manipulated the factor loadings and error variances of the first-order indicators in our measurement model to generate sets of congeneric items, ensuring that the indicators reflected varying degrees of association with the latent constructs. Specifically, the factor loadings for the first, second, and third indicators were fixed at 1.0, 0.9, and 0.75 for both first-order latent variables (i.e., $\lambda_{x_{1}} = \lambda_{m_{1}} = 1.0$, $\lambda_{x_{2}} = \lambda_{m_{2}} = 0.9$, $\lambda_{x_{3}} = \lambda_{m_{3}} = 0.75$). According to equation (11), the error variance of the interaction term was a function of first-order indicators' reliability, suggesting that the interaction effect could be influenced by amount of measurement error. Therefore, we explored how each method performed under three reliability levels: 0.70, 0.80, and 0.90, for low, medium, and high reliablity level. Then the total error variance could be computed, which were $[3.01, \ 1.76, \ 0.78]$ for $[\lambda_{x_{1}}, \ \lambda_{x_{2}}, \ \lambda_{x_{3}}] = [\lambda_{m_{1}}, \ \lambda_{m_{2}}, \ \lambda_{m_{3}}] = [1, \ 0.9, \ 0.75]$, as the reliability was varied at .70, .80, and .90, respectively. At each reliability level, we systematically manipulated the error variance proportions for each indicator, following the proportions suggested by Hsiao et al. (2021), with 44% of the total error variance allocated to the first indicator, 33% to the second, and 23% to the third. For example, under the condition where $\rho = .70$, the error variances for the three indicators were adjusted to $1.32$, $0.99$, and $0.69$, respectively.

With regard to model specification, since MMR relied solely on observed indicators, the model was fitted according to equation (1), where $X$ and $Z$ represented sum scores of mean-centered first-order indicators. In contrast, the latent interaction methods involved more complex model specifications. As suggested by Marsh et al. (2004), we would only include matched-pair UPI in the main study, and therefore $\xi_{x}\xi_{m}$ was indicated by three pairs of PIs (i.e., $x_{1}m_{1}$, $x_{2}m_{2}$, and $x_{3}m_{3}$).^[The all-pair UPI method was also evaluated using the same study design, but only reported as a reference method to matched-pair UPI.]. For the RAPI and 2SPA methods, $\xi_{x}\xi_{m}$ was indicated by a single PI. Specifically, the single PI for RAPI was the mean score of first-order indicators, whereas that for 2S-PA-Int was the Bartlett factor score. To reduce the problem of multicollinearity between first-order latent predictors and the interaction term, the DMC strategy was applied to all the methods.

The methodological literature on latent interaction models exhibited a range of researcher-selected sample sizes from 20 to 5,000 [@chinPartialLeastSquares2003; @linStructuralEquationModels2010b; @chamEstimatingLatentVariable2012a], with common selections ranging from 100 to 500. In this study, we chose N = 100, 250, and 500 to represent small, medium, and large sample sizes, respectively. Since latent variable models may produce unstable estimates especially with small sample size, we set `bounds = TRUE` for both all the four methods to stabilize parameter estimation (Rosseel, 2012). 
As for the correlation between first-order latent predictors, we followed the study design in @hsiaoModelingMeasurementErrors2021 and pre-specified three population correlations $Corr[{\xi_{x},\xi_{m}}]$ (0, 0.3, 0.6) as zero to large correlation. Given that the variances of $y$ (i.e., $\sigma_{y}^2$), $\sigma_{\xi_{x}}^2$, and $\sigma_{\xi_{x}}^2$ were all set to 1, $\psi$ could be computed as $1 - R^2$ in which $R^2 = \gamma_{x}^2 + \gamma_{m}^2 + 2\gamma_{x}\gamma_{m}Corr[{\xi_{x},\xi_{m}}] + \gamma_{xm}^2(1 + Corr[{\xi_{x},\xi_{m}}]^2)$. For instance, $\psi = 1 - (0.3^2 + 0.3^2 + 2\times0.3\times0.3\times0 + 0.3^2\times(1 + 0)^2) = 0.73$ for $Corr[{\xi_{x},\xi_{m}}] = 0$. Similarly, $\psi$ = 0.668 and 0.590 for $Corr[{\xi_{x},\xi_{m}}]$ = 0.3 and 0.6, respectively.

In summary, our study implemented a $3 \times 3 \times 3 \times 2$ factorial design, accommodating variations across three sample sizes, three levels of correlation between first-order latent predictors, three levels of reliability, and two interaction effects (zero and non-zero).

## Evaluation Criteria

We chose widely used evaluation criteria that were summarized across 2,000 replications to evaluate the accuracy and precision of the interaction effect estimates ($\gamma_{xm}$) of the four methods. To facilitate the interpretation of path coefficients, we obtained and evaluated standardized estimates of $\gamma_{x}$, $\gamma_{m}$ and $\gamma_{xm}$.

### Raw Bias and Standardized Bias

Raw bias (RB) refers to the difference between estimated and true parameter values, while standardized bias (SB) normalizes RB using standard error of parameter estimates. This adjustment provides a standardized measure that allows for the comparison of bias across different scales or units of measurement. Given that SB reflects how far an estimate was from its true value in standard error units, it is expected to handle comparisons of models with various parameters (e.g., factor loadings, path coefficients).

In the current study, SB was defined as:

\begin{equation}
SB = \frac{RB(\gamma_{xm})}{SE_{\gamma_{xm}}},
\end{equation}

\begin{equation}
RB(\gamma_{xm}) = R^{-1}\Sigma^{R}_{r = 1}(\hat{\gamma}_{xm_{r}} - \gamma_{xm}),
\end{equation}
where R = 2,000 was the total number of replication cycles. $\hat{\gamma}_{xm_{r}}$ was the estimated interaction effect in each replication cycle $r$ and $\gamma_{xm}$ was the population parameter. $RB(\gamma_{xm})$ was the averaged deviation that $\hat{\gamma}_{xm}$ showed from the population parameter, and $SE_{\gamma_{xm}}$ represented the empirical standard error of $\hat{\gamma}_{xm}$ across replications. @collinsComparisonInclusiveRestrictive2001 suggested that an absolute value of SB $\le 0.40$ would be considered acceptable for each replication condition.

### Robust Relative Standard Error Bias

The relative standard error (SE) bias was used to evaluate precision of $\hat{\gamma}_{xm}$. This criterion compared the empirical standard deviation of $\hat{\gamma}_{xm}$ with the sample estimated SE across replications:

\begin{equation}
Relative\ SE\ Bias = \frac{R^{-1}\Sigma^{R}_{r = 1}(\widehat{SE_{r}} - SD)}{SD},
\end{equation}
where $\widehat{SE}_{r}$ was the estimated standard error of $\hat{\gamma}_{xm}$ in the replication $r$, and $SD$ was the empirical standard deviation of $\hat{\gamma}_{xm}$ obtained from all replications. $SD$ served as a reference measure of variability for $\hat{\gamma}_{xm}$, and a smaller relative SE bias indicated that the estimated standard error was closer to the reference, thereby providing a more accurate measure of the uncertainty in $\hat{\gamma}_{xm}$ across replications. Absolute values of relative SE bias with $\le 10\%$ were considered acceptable and indicated that the standard errors were reasonably unbiased (Hoogland $\&$ Boomsma, 1998). 

Insufficient sample sizes could lead to unreasonably extreme SE values due to increased uncertainty within parameter estimates [@bollenTestingStructuralEquation1993; @byrneStructuralEquationModeling2016]. To avoid inappropriate interpretation of model comparison due to extremely large SE values, a robust version of relative SE bias was calculated and reported:
\begin{equation}
Robust\ Relative\ SE\ Bias = \frac{MDN(\widehat{SE_{r}}) - MAD}{MAD},
\end{equation}
where $MDN$ represented the median value of estimated SE, and $MAD$ denoted empirical median-absolute-deviation values. The MAD was defined by the median of absolute deviations from the median of sample, such that $MAD = b*MDN(|\widehat{SE}_{r} - MDN(SE)|)$ where $b$ was a scale factor set to 1.4826 to match the standard deviation of a normal distribution. Thus MAD could be considered a more consistent estimator for $SD$ [@huberRobustStatistics2011; @rousseeuwAlternativesMedianAbsolute1993]. In the context of biased SE, we did not assume a specific distribution of SE (e.g., normal distribution) in the calculation of robust relative SE bias, and thus used the median of SE estimates due to its robustness to non-normal distributions with skewed data and outliers [@rousseeuwRobustStatisticsOutlier2011]. In summary, MAD measured variability around the median and could serve as a robust substitute to effectively handle outliers and non-normality [@daszykowskiRobustStatisticsData2007]. 

### Outlier Proportion of SE

To provide supplemental information on SE estimates, we included outlier detection using the interquartile range (IQR) method:
\begin{equation}
O_{a} \not\in (Q_{1} - 1.5 \times IQR, \ Q_{3} + 1.5 \times IQR),
\end{equation}
where $O_{a}$ was an observation of outlier for $a$ = 1, 2, ..., b. $IQR$ captured the spread of the middle 50$\%$ of the sample SEs by $IQR = Q_{3} \ - \ Q_{1}$, where $Q_{1}$ and $Q_{3}$ were the 25th percentile and the 75th percentile of the sample. The proportion of outliers was computed as $b/R$, where $b$ represented the total number of outliers, and $R$ was the total number of replications. Similar to the robust relative SE bias, the IQR method did not assume normality and could be considered robust across various distributions [@dekkingModernIntroductionProbability2005a].

### Coverage Rate

The coverage rate of a 95$\%$ confidence interval (CI) was defined as the percentage of replications in which the Wald confidence interval captured the true interaction effect $\gamma_{xm}$. A low coverage rate indicated that the method failed to effectively capture the true interaction effect. A coverage rate larger than $91\%$ was considered acceptable [@muthenHowUseMonte2002].

### Root Mean Squre Error

The root mean square error (RMSE) was used to quantify average magnitude of deviation between the estimated interaction effects and the true value, thereby reflecting both bias and variability of the estimates across replications:
\begin{equation}
RMSE = \sqrt{R^{-1}\Sigma^{R}_{r = 1}(\hat{\gamma}_{xm_{r}} - \gamma_{xm})^2}.
\end{equation}

Methods with averagely lower RMSE were more accurate in estimating $\hat{\gamma}_{xm}$ [@harwellStrategyUsingBias2019]. It should be noted that RMSE provided a comparative metric across methods under the same simulated conditions. 

### Empirical Type I Error Rate and Statistical Power

The empirical type I error informed the probability of incorrectly rejecting the null hypothesis that the latent interaction effect was not significant (i.e., $H_{0}: \gamma_{xm} = 0$) at a specified significance level ($\alpha = .05$). The empirical type I error rate was computed across 2,000 replications by calculating proportion of instances where a Type I error occurred. An empirical Type I error rate within the range of approximately 0.025 to 0.075 was widely considered acceptable, showing that the statistical tests were robust (Bradley, 1978). In contrast, statistical power represented a method’s capacity to detect a true effect. In this study, it was defined as the proportion of correctly rejecting the null hypothesis when the interaction effect truly exists (i.e., $H_{a}: \gamma_{xm} = 0.3$). 

# Results

## Convergence Rate and Warning Messages

Errors during model estimation could lead to replication failures and affect convergence rates. The convergence rate, defined as the proportion of replications completed without estimation errors, was calculated across all replication attempts. For the MMR and RAPI methods, convergence was consistently achieved at a rate of 100% across all conditions, indicating that no estimation errors were encountered. Similarly, matched-pair UPI demonstrated 100% convergence rates in most conditions except for one case with a small sample size (i.e., $\textit{N} = 100$), where the rate dropped slightly to 99.95%. In contrast, 2S-PA-Int showed more variability in convergence rates, ranging from 98.91% to 99.95%, with at least one error observed in ten different small sample conditions. 

In addition to the replication failures, warning messages could appear despite successful convergence. These warnings, which included negative variance estimates and non-positive definite covariance matrices, indicated potential issues with extreme or unstable estimates that could affect interpretation of model results. The proportions of warning messages were similarly computed. Specifically, MMR did not generate any warning across all conditions. RAPI and 2S-PA-Int showed low warning incidence, with maximum rates of 0.70% and 0.30% respectively, across up to six small sample size conditions. Matched-pair UPI demonstrated the highest frequency of warnings across 32 conditions, particularly under small sample sizes and low reliability, with warning rates ranging from 0.05% to 14.82%.

```{r convergence rate, message=FALSE, warning=FALSE}
sim_results <- sim %>% 
  dplyr::select(-c(REPLICATIONS:COMPLETED))

errors <- sim_results[ ,c("N",
                             "cor_xm",
                             "rel",
                             "gamma_xm",
                             "ERRORS")]
errors_msg <- SimExtract(sim_results, what = "errors")
warnings <- sim_results[ ,c("N",
                             "cor_xm",
                             "rel",
                             "gamma_xm",
                             "warning_total.mmr.warnings_count",
                             "warning_total.upi.warnings_count",
                             "warning_total.rapi.warnings_count",
                             "warning_total.tspa.warnings_count",
                             "WARNINGS")]
warnings_msg <- SimExtract(sim_results, what = "warnings")

# Error rate
errors_rate <- errors %>%
  mutate(converge_rate = 100 - round(ERRORS/(ERRORS + 2000), 4)*100)
# Warning rate
warnings_rate <- warnings %>%
  mutate(total_rate = round(WARNINGS/(WARNINGS + 2000), 4)*100,
         mmr_rate = round(warning_total.mmr.warnings_count/
                                  (warning_total.mmr.warnings_count + 2000), 4)*100,
         upi_rate = round(warning_total.upi.warnings_count/
                                  (warning_total.upi.warnings_count + 2000), 4)*100,
         rapi_rate = round(warning_total.rapi.warnings_count/
                                   (warning_total.rapi.warnings_count + 2000), 4)*100,
         tspa_rate = round(warning_total.tspa.warnings_count/
                                   (warning_total.tspa.warnings_count + 2000), 4)*100)
```

## Raw Bias and Standardized Bias for $\gamma_{xm}$

As outlined in Table 1, an examination of all simulation conditions, including both zero ($\gamma_{xm} = 0$) and non-zero ($\gamma_{xm} = 0.3$) interaction effects, revealed that the absolute values of standardized bias (SB) for the estimates of $\gamma_{xm}$ across the latent interaction methods consistently remained within the acceptable threshold of .40, ranging from 0.00 to 0.20. Similarly, raw bias (RB) values were relatively small, with absolute values ranging from 0.00 to 0.10. 

When the interaction effect was zeo, the SB and RB values did not exhibit much variation across methods and conditions, indicating that all methods demonstrated good performance in estimating interaction effects with accuracy.

For non-zero effects, MMR was notably less comparable to the latent interaction methods, as it yielded substantially larger magnitude of RB and SB, particularly under conditions of low ($\rho = 0.7$) and medium ($\rho = 0.8$) item reliability. Most SB values exceeded the threshold of 0.40, indicating that MMR was ineffective of handling measurement error. 

In contrast, for the latent interaction methods, as item reliability increased, the magnitude of SB and RB decreased for all three methods, indicating that their estimation of interaction effects became progressively more accurate as measurement error in the first-order indicators diminished. A similar decreasing trend was observed for sample size. Specifically, SB and RB generally became smaller as sample size increased, which aligned with statistical property of SEM models.

The absolute SB values for all the latent interaction methods were predominantly positive, with the exception of matched-pair UPI and 2S-PA-Int under some conditions of high reliability and large sample size. These findings aligned with prior research on RAPI and matched-pair UPI, which demonstrated a tendency to overestimate interaction effects, particularly in conditions of low reliability (Marsh et al., 2004; Hsiao et al., 2018). The magnitude of SB values was generally larger for RAPI (ranging from 0.03 to 0.20) compared to matched-pair UPI (ranging from -0.03 to 0.14) and 2S-PA-Int (ranging from -0.03 to 0.10), indicating that RAPI tended to yield more upward bias across these conditions.

Overall, the latent interaction methods yielded comparably low and acceptable standardized biases across simulation conditions.

```{r standardized bias (raw bias), message=FALSE, warning=FALSE}
# Helper Function
bold_if <- function(cell) {
  first_number <- as.numeric(str_extract(cell, "^[^\\(]+"))
  if (!is.na(first_number) && abs(first_number) > 0.4) {
    return(sprintf("\\textbf{%s}", cell))
  } else {
    return(cell)
  }
}

# Null Effect
rb_null <- sim_null %>%
  dplyr::select(N:raw_bias.tspa.est) %>%
  dplyr::select(-gamma_xm) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(rb_null) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

rb_null_wide <- rb_null %>%
  pivot_wider(names_from = `$\\rho$`,
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), ~ sprintf("%.2f", .)))
names(rb_null_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

sb_null <- sim_null %>% 
  dplyr::select(N:rel, std_bias.mmr.est:std_bias.tspa.est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) 
names(sb_null) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

sb_null_wide <- sb_null %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), ~ sprintf("%.2f", .)))
names(sb_null_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

rbsb_null <- rb_null_wide
for (col_idx in 3:ncol(rb_null_wide)) {
  combined_values <- mapply(FUN = function(x, y) paste(y, "\ (", x, ")", sep = ""),
                            x = rb_null_wide[, col_idx], y = sb_null_wide[, col_idx])
  rbsb_null[, col_idx] <- combined_values
}

for (i in 3:ncol(rbsb_null)) {
  rbsb_null[[i]] <- sapply(rbsb_null[[i]], bold_if)
}

# Alternative Effect
rb_alt <- sim_alt %>%
  dplyr::select(N:raw_bias.tspa.est) %>%
  dplyr::select(-gamma_xm) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(rb_alt) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

rb_alt_wide <- rb_alt %>%
  pivot_wider(names_from = `$\\rho$`,
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), ~ sprintf("%.2f", .)))
names(rb_alt_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

sb_alt <- sim_alt %>% 
  dplyr::select(N:rel, std_bias.mmr.est:std_bias.tspa.est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) 
names(sb_alt) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

sb_alt_wide <- sb_alt %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), ~ sprintf("%.2f", .)))
names(sb_alt_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

rbsb_alt <- rb_alt_wide
for (col_idx in 3:ncol(rb_alt_wide)) {
  combined_values <- mapply(FUN = function(x, y) paste(y, "\ (", x, ")", sep = ""),
                            x = rb_alt_wide[, col_idx], y = sb_alt_wide[, col_idx])
  rbsb_alt[, col_idx] <- combined_values
}

for (i in 3:ncol(rbsb_alt)) {
  rbsb_alt[[i]] <- sapply(rbsb_alt[[i]], bold_if)
}

# Table
rbsb_table <- apa_table(list(`$\\gamma_{xm} = 0$` = rbsb_null,
                             `$\\gamma_{xm} = 0.3$` = rbsb_alt),
                        merge_method = "table_spanner",
                        escape = F,
                        caption = "Standardized Bias and Raw Bias of Latent Interaction Estimates ($\\gamma_{xm}$) Across 2,000 Replications.",
                        align = c(rep("c", ncol(rbsb_alt))),
                        col_spanners = list(`MMR` = c(3, 5), `Matched-Pair UPI` = c(6, 8),
                                            `RAPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
                        landscape = TRUE,
                        font_size = "tiny",
                        note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; $\\gamma_{xm} = 0$ indicates no latent interaction effect; $\\gamma_{xm} = 0.3$ indicates a non-zero interaction effect; MMR = moderated multiple regression; Matched-Pair UPI = matched-pair product unconstrained indicator; RAPI = reliability-adjusted product indicator; 2S-PA-Int = two-stage path analysis with interaction. Values in parentheses indicate raw bias. All numerical values are rounded to two decimal places for consistency. Note that values close to zero are displayed as 0.00, with negative signs maintained to indicate the direction of bias. Besides, values exceeding the recommended threshold (0.40) are bolded.")

rbsb_table
```

## Relative SE Bias of $\gamma_{xm}$

Table 2 presents the robust relative standard error (SE) bias ratio along with the proportions of SE outliers. Values outside the -10% to 10% range were bolded for emphasis. Overall, the relative SE bias for both MMR and the latent interaction methods remained within this range for the zero effect condition, and no discernible pattern was observed from distribution of bias across simulation conditions.

For the non-zero effects, the relative SE bias for MMR frequently exceeded the acceptable range and showed notable downward bias in several conditions, ranged from -17.95% to -1.83%. It suggested that MMR consistently underestimated standard errors of interaction effect estimates, which might lead to potentially misleading inferences.

RAPI, matched-pair UPI, and 2S-PA-Int generally maintained relative SE biases within the acceptable -10% to 10% range under medium ($\rho = 0.80$) and high ($\rho = 0.90$) reliability conditions. However, matched-pair UPI had two instances of bias exceeding the threshold in small sample size and low reliability conditions, with values of -13.37% and -15.60%. RAPI displayed unacceptable relative SE biases in three low-reliability conditions ($\rho = 0.70$), even with large sample sizes, while 2S-PA-Int had only one instance under small sample size and low reliability. No clear pattern of relative SE bias was observed across reliability ($\rho$) and sample size (N). Overall, the relative SE bias tended to be negative for matched-pair UPI and 2S-PA-Int, indicating underestimation of SEs, while RAPI showed positive biases, indicating overestimated SEs.

The outlier proportions of SEs exhibited a clear declining trend across all methods as sample size increased and reliability levels improved, indicating more accurate and stable estimates of $\gamma_{xm}$ with fewer extreme SE values. Notably, MMR consistently showed lower outlier proportions compared to the latent interaction methods across all conditions, for both zero and non-zero interaction effects, suggesting that MMR produced fewer extreme SE estimates overall.

```{r MAD relative SE bias with outliers proportion, message=FALSE, warning=FALSE}
# Helper Function
bold_if_larger_than_10 <- function(cell) {
  first_number <- as.numeric(str_extract(cell, "^[^\\(]+"))
  if (!is.na(first_number) && abs(first_number) > 10) {
    return(sprintf("\\textbf{%s}", cell))
  } else {
    return(cell)
  }
}

# Null Effect
MAD_null <- sim_null %>% 
  dplyr::select(N:rel, stdMed_rse_bias.mmr.se_std:stdMed_rse_bias.tspa.se_std) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(stdMed_rse_bias.mmr.se_std = stdMed_rse_bias.mmr.se_std*100,
         stdMed_rse_bias.upi.se_std = stdMed_rse_bias.upi.se_std*100,
         stdMed_rse_bias.rapi.se_std = stdMed_rse_bias.rapi.se_std*100,
         stdMed_rse_bias.tspa.se_std = stdMed_rse_bias.tspa.se_std*100)
names(MAD_null) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

MAD_null_wide <- MAD_null %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), 
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), ~ sprintf("%.2f", .)))
names(MAD_null_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

outse_null <- sim_null %>% 
  dplyr::select(N:rel, outlier_se.mmr.se_std:outlier_se.tspa.se_std) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(across(where(is.numeric), ~sprintf("%.2f", .)))
names(outse_null) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

outse_null_wide <- outse_null %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), ~ sprintf("%.2f", .))) 
names(outse_null_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

MAD_null <- MAD_null_wide
for (col_idx in 3:ncol(MAD_null_wide)) {
  combined_values <- mapply(FUN = function(x, y) paste(y, "\ (", x, ")", sep = ""),
                            x = outse_null_wide[, col_idx], y = MAD_null_wide[, col_idx])
  MAD_null[, col_idx] <- combined_values
}

for (i in 3:ncol(MAD_null)) {
  MAD_null[[i]] <- sapply(MAD_null[[i]], bold_if_larger_than_10)
}

# Alternative Effect
MAD_alt <- sim_alt %>% 
  dplyr::select(N:rel, stdMed_rse_bias.mmr.se_std:stdMed_rse_bias.tspa.se_std) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(stdMed_rse_bias.mmr.se_std = stdMed_rse_bias.mmr.se_std*100,
         stdMed_rse_bias.upi.se_std = stdMed_rse_bias.upi.se_std*100,
         stdMed_rse_bias.rapi.se_std = stdMed_rse_bias.rapi.se_std*100,
         stdMed_rse_bias.tspa.se_std = stdMed_rse_bias.tspa.se_std*100)
names(MAD_alt) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

MAD_alt_wide <- MAD_alt %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), 
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), ~ sprintf("%.2f", .)))
names(MAD_alt_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

outse_alt <- sim_alt %>% 
  dplyr::select(N:rel, outlier_se.mmr.se_std:outlier_se.tspa.se_std) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(across(where(is.numeric), ~sprintf("%.2f", .)))
names(outse_alt) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

outse_alt_wide <- outse_alt %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), ~ sprintf("%.2f", .))) 
names(outse_alt_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

MAD_alt <- MAD_alt_wide
for (col_idx in 3:ncol(MAD_alt_wide)) {
  combined_values <- mapply(FUN = function(x, y) paste(y, "\ (", x, ")", sep = ""),
                            x = outse_alt_wide[, col_idx], y = MAD_alt_wide[, col_idx])
  MAD_alt[, col_idx] <- combined_values
}

for (i in 3:ncol(MAD_alt)) {
  MAD_alt[[i]] <- sapply(MAD_alt[[i]], bold_if_larger_than_10)
}

# Table
MAD_table <- apa_table(
  list(`$\\gamma_{xm} = 0$` = MAD_null,
       `$\\gamma_{xm} = 0.3$` = MAD_alt), 
  merge_method = "table_spanner",
            escape = F,
            caption = "Robust Relative Standard Error (SE) Bias Ratio and Outlier Proportion of SE ($\\%$) of Latent Interaction Estimates ($\\gamma_{xm}$) Across 2,000 Replications.",
            align = c(rep("c", ncol(MAD_alt))),
            col_spanners = list(`MMR` = c(3, 5), `Matched-Pair UPI` = c(6, 8),
                                `RAPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
            landscape = TRUE,
            font_size = "tiny",
            note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; $\\gamma_{xm} = 0$ indicates no latent interaction effect; $\\gamma_{xm} = 0.3$ indicates a non-zero interaction effect; MMR = moderated multiple regression; Matched-Pair UPI = matched-pair product unconstrained indicator; RAPI = reliability-adjusted product indicator; 2S-PA-Int = two-stage path analysis with interaction. Values in parentheses represent the outlier proportions of SE, given as percentages. Relative SE bias values outside the acceptable range of [-10$\\%$, 10$\\%$] are bolded.")

MAD_table
```

## Coverage Rate of 95% CI of $\gamma_{xm}$

As shown in Table 3, when the interaction effect was zero, the coverage rates of the 95% confidence interval (CI) for MMR and the latent interaction methods were all above the acceptable threshold of 91%. Specifically, RAPI and matched-pair UPI produced generally higher coverage rates than 2S-PA-Int and MMR across sample size and reliability conditions, with a range from 95.30% to 99.30% for RAPI, and 95.00% to 99.30% for matched-pair UPI. 

When the interaction effect was non-zero, RAPI and 2S-PA-Int maintained coverage rates all falling within the acceptable range across all conditions, with a range from 95.35% to 97.45% for RAPI and from 93.65% to 95.05% for 2S-PA-Int. Matched-pair UPI yielded below-threshold coverage rates under four conditions with small ($\textit{N} = 100$) or medium ($\textit{N} = 250$) sample size, and low reliability ($\rho = 0.70$). Similar for the zero effect, RAPI continued to outperform matched-pair UPI and 2S-PA-Int in terms of coverage rates across all conditions. In contrast, MMR exhibited unsatisfactory coverage rates for nearly all conditions, ranging from 36.4% to 91.3%, which indicated that the model without accounting for measurement error was not able to effectively capture true interaction effects. 

No clear trend in coverage rates was observed within methods regarding sample size, population reliability levels, or the correlation between first-order latent variables. Nonetheless, RAPI consistently demonstrated the highest coverage rate among the latent interaction methods, followed by 2S-PA-Int and matched-pair UPI. This pattern suggested that RAPI showed the greatest likelihood of capturing the true interaction effect when such an effect was present.

```{r coverage rate, message=FALSE, warning=FALSE}
# Helper Function
bold_if_less_than_91 <- function(cell) {
  if (!is.na(cell) && abs(cell) < 91) {
    return(sprintf("\\textbf{%s}", cell))
  } else {
    return(cell)
  }
}

# Null Effect
cov_null <- sim_null %>% 
  dplyr::select(N:rel, coverage_std.mmr.est:coverage_std.tspa.est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(coverage_std.mmr.est = coverage_std.mmr.est*100,
         coverage_std.upi.est = coverage_std.upi.est*100,
         coverage_std.rapi.est = coverage_std.rapi.est*100,
         coverage_std.tspa.est = coverage_std.tspa.est*100) 
names(cov_null) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

cov_null_wide <- cov_null %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(cov_null_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

for (i in 3:ncol(cov_null_wide)) {
  # Applying the formatting function to each element of the column
  cov_null_wide[[i]] <- sapply(cov_null_wide[[i]], bold_if_less_than_91)
}

# Alternative Effect
cov_alt <- sim_alt %>% 
  dplyr::select(N:rel, coverage_std.mmr.est:coverage_std.tspa.est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(coverage_std.mmr.est = coverage_std.mmr.est*100,
         coverage_std.upi.est = coverage_std.upi.est*100,
         coverage_std.rapi.est = coverage_std.rapi.est*100,
         coverage_std.tspa.est = coverage_std.tspa.est*100) 
names(cov_alt) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2SPA")

cov_alt_wide <- cov_alt %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(cov_alt_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

for (i in 3:ncol(cov_alt_wide)) {
  # Applying the formatting function to each element of the column
  cov_alt_wide[[i]] <- sapply(cov_alt_wide[[i]], bold_if_less_than_91)
}

cov_table <- apa_table(
  list(`$\\gamma_{xm} = 0$` = cov_null_wide,
       `$\\gamma_{xm} = 0.3$` = cov_alt_wide), 
  merge_method = "table_spanner",
            escape = F,
            caption = "Coverage Rate of 95 $\\%$ Confidence Interval (CI) of Latent Interaction Estimates ($\\gamma_{xm}$) Across 2,000 Replications.",
            align = c(rep("c", ncol(cov_alt))),
            col_spanners = list(`MMR` = c(3, 5), `Matched-Pair UPI` = c(6, 8),
                                `RAPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
            landscape = TRUE,
            font_size = "tiny",
            note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; $\\gamma_{xm} = 0$ indicates no latent interaction effect; $\\gamma_{xm} = 0.3$ indicates a non-zero interaction effect; MMR = moderated multiple regression; Matched-Pair UPI = matched-pair product unconstrained indicator; RAPI = reliability-adjusted product indicator; 2S-PA-Int = two-stage path analysis with interaction. Coverage rates below the acceptable threshold of 91$\\%$ are bolded.")

cov_table
```

## RMSE of $\gamma_{xm}$

Table 4 exhibited that, for both zero and non-zero interaction effects, the RMSE values consistently decreased as sample size and reliability level increased for all methods. The point estimates of $\gamma_{xm}$ for MMR generally showed smaller RMSE compared to the latent interaction methods. 

The 2S-PA-Int method among the latent interaction methods showed the lowest (or equally lowest) RMSE values across all the conditions. For instance, under conditions of small sample size and low reliability, the RMSE values for 2S-PA-Int ranged from 0.23 to 0.26, while those for RAPI and matched-pair UPI ranged from 0.37 to 0.66 and 0.33 to 0.60, respectively. Notably, as reliability increased, discrepancies in RMSE values across methods became less apparent, indicating the performance of all methods converged as measurement error diminished.

```{r rmse, message=FALSE, warning=FALSE}
# Null Effect
rmse_null <- sim_null %>% 
  dplyr::select(N:rel, rmse.mmr.est:rmse.tspa.est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(rmse_null) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2S-PA-Int")

rmse_null_wide <- rmse_null %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2S-PA-Int"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(rmse_null_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

# Alternative Effect
rmse_alt <- sim_alt %>% 
  dplyr::select(N:rel, rmse.mmr.est:rmse.tspa.est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(rmse_alt) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2S-PA-Int")

rmse_alt_wide <- rmse_alt %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2S-PA-Int"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(rmse_alt_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

rmse_table <- apa_table(
  list(`$\\gamma_{xm} = 0$` = rmse_null_wide,
       `$\\gamma_{xm} = 0.3$` = rmse_alt_wide), 
  merge_method = "table_spanner",
            escape = F,
            caption = "Root Mean Square Error (RMSE) of Latent Interaction Estimates ($\\gamma_{xm}$) Across 2,000 Replications.",
            align = c(rep("c", ncol(rmse_alt_wide))),
            col_spanners = list(`MMR` = c(3, 5), `Matched-Pair UPI` = c(6, 8),
                                `RAPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
            landscape = TRUE,
            font_size = "tiny",
            note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; $\\gamma_{xm} = 0$ indicates no latent interaction effect; $\\gamma_{xm} = 0.3$ indicates a non-zero interaction effect; MMR = moderated multiple regression method; Matched-Pair UPI = matched-pair product unconstrained indicator; RAPI = reliability-adjusted product indicator method; 2S-PA-Int = two-stage path analysis with interaction.")

rmse_table
```

## Empirical Type I Error Rate and Statistical Power

Empirical Type I error rates for zero interaction effects, calculated as the proportion of times the null hypothesis ($\gamma_{xm} = 0$) was incorrectly rejected, ranged from 0.02 to 0.06 across all methods. While differences between methods were modest, MMR consistently exceeded the critical value ($\alpha = 0.05$) in conditions with low and medium sample sizes. Among the latent interaction methods, RAPI and 2S-PA-Int also occasionally exceeded the critical threshold under similar conditions, whereas matched-pair UPI remained consistently below the threshold. The results indicated that matched-pair UPI was the most conservative in avoiding false positive cases, though 2S-PA-Int and RAPI also maintained acceptable performance.

Regarding statistical power, MMR displayed higher power than the latent interaction methods in small and medium sample sizes. However, this advantage diminished as sample size increased to large ($\textit{N} = 500$) and item reliability improved to 0.9. Among the latent interaction methods, 2S-PA-Int exhibited the highest power for detecting true non-zero interaction effects under conditions of small sample size and low reliability, with power ranging from 0.48 to 0.71. RAPI followed, with power ranging from 0.31 to 0.56, while matched-pair UPI showed the lowest power, ranging from 0.23 to 0.48. As the sample size increased, all methods performed similarly well, and the differences in power across methods became negligible.

```{r type I error, message=FALSE, warning=FALSE}
type1 <- sim_null %>% 
  dplyr::select(N:rel, type1_lrt.mmr.est_usd:type1_lrt.tspa.est_usd) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(type1) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2S-PA-Int")

type1_wide <- type1 %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2S-PA-Int"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(type1_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

# type1_table <- apa_table(type1_wide, 
#             escape = F,
#             caption = "Empirical Type I Error Rate for $\\gamma_{xm} (= 0)$ over 2,000 Replications.",
#             align = c(rep("c", ncol(type1))),
#             col_spanners = list(`MMR` = c(3, 5), `Matched-Pair UPI` = c(6, 8),
#                                 `RAPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
#             landscape = TRUE,
#             font_size = "footnotesize",
#             note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; $\\gamma_{xm} = 0$ indicates no latent interaction effect; $\\gamma_{xm} = 0.3$ indicates a non-zero interaction effect; MMR = moderated multiple regression; Matched-Pair UPI = matched-pair product unconstrained indicator; RAPI = reliability-adjusted product indicator; 2S-PA-Int = two-stage path analysis with interaction.")
# 
# type1_table
```

```{r power, message=FALSE, warning=FALSE}
power <- sim_alt %>% 
  dplyr::select(N:rel, power_lrt.mmr.est_usd:power_lrt.tspa.est_usd) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(power) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "Matched-Pair UPI", "RAPI", "2S-PA-Int")

power_wide <- power %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "Matched-Pair UPI", "RAPI", "2S-PA-Int"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(power_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

type1_power_table <- apa_table(
  list(`Empirical Type I Error Rate ($\\gamma_{xm} = 0$)` = type1_wide,
       `Statistical Power ($\\gamma_{xm} = 0.3$)` = power_wide), 
  merge_method = "table_spanner",
            escape = F,
            caption = "Empirical Type I Error Rate and Statistical Power Across 2,000 Replications.",
            align = c(rep("c", ncol(power_wide))),
            col_spanners = list(`MMR` = c(3, 5), `Matched-Pair UPI` = c(6, 8),
                                `RAPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
            landscape = TRUE,
            font_size = "tiny",
            note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; $\\gamma_{xm} = 0$ indicates no latent interaction effect; $\\gamma_{xm} = 0.3$ indicates a non-zero interaction effect; MMR = moderated multiple regression; Matched-Pair UPI = matched-pair product unconstrained indicator; RAPI = reliability-adjusted product indicator; 2S-PA-Int = two-stage path analysis with interaction.")

type1_power_table
```

# Empirical Demonstration Using Real Data

In this section, we applied and compared the three latent interaction methods by replicating the findings from Park's (2011) study, which examined the interaction between intrinsic motivation (IM) and extrinsic motivation (EM) on reading performance using hierarchical linear modeling. Park’s original analysis identified a significant interaction effect, indicating that the influence of IM on reading scores varied according to the level of EM. While factor scores for the two motivation constructs were utilized as explanatory variables in Park's study, the interaction effects at the item level were not explored. To address this limitation, we replicated the study using latent interaction methods with a focus on observed items. A visual representation of the interaction model is provided in Figure 1.

The data for the original study was sourced from the Progress in International Reading Literacy Study (PIRLS) 2006, a global assessment of reading literacy among fourth-grade students (Mullis et al., 2007). Park (2011) specifically analyzed the United States sample, which represented fourth-grade students from all 50 states and the District of Columbia. A notable concern with this dataset was the poor reliability of the observed items measuring EM, with a reported Cronbach’s alpha of $\alpha_{EM} = 0.50$, while IM had a reliability barely meeting the acceptable threshold ($\alpha_{IM} = 0.70$). Although low reliability in EM might not pose significant issues in analyses based solely on observed items, it could lead to biased and unstable estimates when applied to latent interaction analyses. 

Considering that observed items with poor reliability were unsuitable for latent interaction methods, we instead used the Croatia sample in the PIRLS 2021 study (Von Davier et al., 2023). The initial sample comprised 1,226 participants; after excluding those with missing responses on any of the observed motivation items, the final sample included 1,136 students. As multilevel and multi-group analyses were not focus of this demonstration, we conducted and reported only student-level analyses. The reliability of the IM and EM constructs in the Croatian sample was satisfactory, with Cronbach’s alpha of $\alpha_{IM} = 0.83$ and $\alpha_{EM} = 0.80$.

Six observed items in the Croatian sample were identified as relevant to the motivation constructs, with three items assessing IM (i.e., "I would like to have more time for reading," "I think reading is boring," "I enjoy reading") and three items measuring EM (i.e., "I like talking about books with other people," "I would be happy if someone gave me a book as a present," "I learn a lot from reading").^[The items "I would like to have more time for reading" and "I learn a lot from reading" were selected to replace "I read only if I have to" and "I need to read well for my future" in the original anlayses, as the latter two were not included in the PIRLS 2021 questionnaire.]. All items were rated on a four-point Likert scale, ranging from 1 ("disagree a lot") to 4 ("agree a lot"). To avoid computational inconsistencies and to ensure uniform interpretation, five items were recoded such that higher scores uniformly reflected greater levels of reading motivation.

To replicate the findings of Park (2011), we hypothesized that EM would be negatively related to students' reading performance, IM would be positively related, and a significant interaction would exist between the two types of reading motivation. The point estimates of path coefficients, along with their standard errors and significance levels, were reported for method comparison. The PIRLS 2021 data utilized five plausible values to accurately assess students' reading performance, addressing the substantial uncertainty in estimating individual characteristics (Mullis et al., 2023). Following the guidelines in the PIRLS 2021 technical report, the latent interaction model was fitted separately for each plausible value as the dependent variable in each latent interaction method, and the estimates were subsequently combined using Rubin's rules.^[A detailed treatment of the use of plausible values can be found in PIRLS 2021 Technical Report (Mullis et al., 2023).]

A two-factor measurement model was fitted to assess the structure of the motivation constructs. The fit indices indicated an acceptable fit to the data: $\chi^2 = 58.26$ with $\textit{df} = 8$, $CFI = .98$, $TLI = .97$, $RMSEA = .07$, and $SRMR = .03$. Although the significant $\chi^2$ suggested a notable discrepancy between the observed and model-implied covariance matrices, the sensitivity of $\chi^2$ to large sample sizes often results in significant values even for minor discrepancies. Therefore, greater emphasis should be placed on comparative fit indices [@huCutoffCriteriaFit1999]. Specifically, both $CFI$ and $TLI$ indicated a good fit (> .95), while $RMSEA$ and $SRMR$ remained below the commonly accepted thresholds of .08 and .05, respectively [@browneAlternativeWaysAssessing1992; @joreskogLISRELStructuralEquation1993]. Overall, these results demonstrate that the measurement model adequately fits the data. At this stage, the data quality was deemed sufficient for the application of latent interaction methods.

Table 6 presented the point estimates of the path coefficients, standard errors, and p-values for each of the three methods, although the first-order effects (i.e., $\hat{\beta}_{IM}$ and $\hat{\beta}_{EM}$) were not the primary focus of this study. Notably, all estimates were based on standardized path coefficients to ensure comparability of magnitude across methods. Consistent with the hypotheses and Park’s (2011) findings, higher levels of IM were positively associated with increased reading performance scores (all $\textit{p}$ values $< .05$), whereas higher levels of EM were negatively related to performance (all $\textit{p}$ values $< .05$) across methods. Regarding the interaction effect, a significant associaition was found between the latent interaction term and performance scores (all $\textit{p}$ values $< .05$), indicating that the effect of one motivation construct on reading performance was contingent upon the level of the other.

Notably, matched-pair UPI and 2S-PA-Int produced comparable parameter estimates for both the first-order and interaction effects (e.g., 0.88 and 0.86 for $\hat{\beta}_{IM}$; -0.89 and -0.87 for $\hat{\beta}_{EM}$; -0.16 and -0.15 for $\hat{\beta}_{IM \times EM}$). In contrast, RAPI consistently yielded larger magnitude estimates for these effects compared to matched-pair UPI and 2S-PA-Int. Additionally, RAPI produced slightly higher standard error estimates for both first-order and interaction effects than the other two methods. The empirical results were consistent with the simulation findings, such that 2S-PA-Int generally exhibited the lowest standardized bias for the latent interaction estimates when reliability was 0.80, followed by matched-pair UPI and RAPI. Regarding standard errors, the positive relative SE bias observed for RAPI in the simulation results corresponded with its relatively higher SE estimates in this empirical example. 

```{r PIRL2021 Example, message=FALSE, warning=FALSE, include=FALSE}
PIRLS2021 <- read_sav(here("Paper", "PIRLS2021", "ASGHRVA5.sav"))

PIRLS_Data <- PIRLS2021 %>%
  select(ASBR07A:ASBR07H, ASRREA01:ASRREA05) %>%
  drop_na()

PIRLS_Data <- PIRLS_Data %>%
  mutate(across(
    .cols = c(ASBR07A:ASBR07B, ASBR07D:ASBR07H),
    .fns = ~ recode(as.numeric(.x), `1` = 4, `2` = 3, `3` = 2, `4` = 1),
    .names = "{.col}_recode"
  )) %>%
  drop_na()

# Measurement Model
IM <- PIRLS_Data %>% 
  select(ASBR07C, ASBR07D_recode, ASBR07E_recode) %>%
  rename(
    IM1 = ASBR07C,
    IM2 = ASBR07D_recode,
    IM3 = ASBR07E_recode
  )

EM <- PIRLS_Data %>% 
  select(ASBR07A_recode, ASBR07B_recode, ASBR07F_recode) %>%
  rename(
    EM1 = ASBR07A_recode,
    EM2 = ASBR07B_recode,
    EM3 = ASBR07F_recode
  )

model <- "IM =~ IM1 + IM2 + IM3
          EM =~ EM1 + EM2 + EM3"

DVs <- c("ASRREA01", "ASRREA02", "ASRREA03", "ASRREA04", "ASRREA05")
final_df <- cbind(IM, EM, PIRLS_Data[DVs])

fit <- fitmeasures(sem(model, final_df))

# UPI
intNames <- paste0(colnames(IM), colnames(EM))
mod_df <- indProd(final_df, 
                  var1 = colnames(IM),
                  var2 = colnames(EM),
                  match = TRUE, 
                  meanC = T, 
                  residualC = F, 
                  doubleMC = T,
                  namesProd = intNames) # using the DMC strategy
upi_df <- cbind(mod_df, PIRLS_Data[DVs])
mod_upi <- "
          IM =~ IM1 + IM2 + IM3 
          EM =~ EM1 + EM2 + EM3
          Int =~ IM1EM1 + IM2EM2 + IM3EM3

          DV ~ b1*IM + b2*EM + b3*Int
              
          # Variance
          IM ~~ v1*IM
          EM ~~ v2*EM
          Int ~~ v3*Int
          
          # Covariance
          IM ~~ cov_12*EM
          IM ~~ cov_13*Int
          EM ~~ cov_23*Int
          
          # Disturbance
    			DV ~~ dist_y*DV
          var_y :=  dist_y + (b1^2 * v1 + b2^2 * v2 + b3^2 * v3 + 
                              2 * b1 * b2 * cov_12 + 
                              2 * b1 * b3 * cov_13 + 
                              2 * b2 * b3 * cov_23)
          
          beta1 := b1*sqrt(v1)/sqrt(var_y)
          beta2 := b2*sqrt(v2)/sqrt(var_y)
          beta3 := b3*sqrt(v1)*sqrt(v2)/sqrt(var_y)
          "

# Fit the model using upi()
beta1_upi <- c()
beta2_upi <- c()
beta3_upi <- c()
se1_upi <- c()
se2_upi <- c()
se3_upi <- c()
p1_upi <- c()
p2_upi <- c()
p3_upi <- c()

for (dv in DVs) {
  mod_upi_temp <- gsub("DV", dv, mod_upi)  # Replace "DV" with the current dv in the loop
  fit_upi <- sem(mod_upi_temp, upi_df)
  beta1_upi <- unname(c(beta1_upi, coef(fit_upi, type = "user")["beta1"]))
  beta2_upi <- unname(c(beta2_upi, coef(fit_upi, type = "user")["beta2"]))
  beta3_upi <- unname(c(beta3_upi, coef(fit_upi, type = "user")["beta3"]))
  se1_upi <- c(se1_upi, sqrt(vcov(fit_upi, type = "user")["beta1", "beta1"]))
  se2_upi <- c(se2_upi, sqrt(vcov(fit_upi, type = "user")["beta2", "beta2"]))
  se3_upi <- c(se3_upi, sqrt(vcov(fit_upi, type = "user")["beta3", "beta3"]))
}

beta1_upi_avg <- floor(mean(beta1_upi) * 100) / 100
beta2_upi_avg <- floor(mean(beta2_upi) * 100) / 100
beta3_upi_avg <- floor(mean(beta3_upi) * 100) / 100

# Merge SE
combine_se <- function(combined_path, se_val) {
  within_var <- mean(se_val^2)
  between_var <- var(combined_path)
  m <- length(combined_path)
  total_variance <- within_var + (1 + 1/m) * between_var
  return(sqrt(total_variance))
}

se1_upi_avg <- combine_se(beta1_upi, se1_upi)
se2_upi_avg <- combine_se(beta2_upi, se2_upi)
se3_upi_avg <- combine_se(beta3_upi, se3_upi)

# Merge P-values
combine_p_value <- function(avg_path, combined_se) {
  combined_z <- avg_path / combined_se
  combined_p_value <- 2 * (1 - pnorm(abs(combined_z)))
  return(combined_p_value)
}

p1_upi_avg <- combine_p_value(beta1_upi_avg, se1_upi_avg)
p2_upi_avg <- combine_p_value(beta2_upi_avg, se2_upi_avg)
p3_upi_avg <- combine_p_value(beta3_upi_avg, se3_upi_avg)

# RAPI
# df
rapi_df <- final_df %>%
  mutate(across(c(IM1:EM3), ~ . - mean(.)))
rapi_df$IM_m <- rowSums(rapi_df[, paste0("IM", 1:3)])/3
rapi_df$EM_m <- rowSums(rapi_df[, paste0("EM", 1:3)])/3
rapi_df$Int <- rapi_df$IM_m*rapi_df$EM_m
IM_rel <- psych::alpha(rapi_df[, paste0("IM", 1:3)])$total$raw_alpha
EM_rel <- psych::alpha(rapi_df[, paste0("EM", 1:3)])$total$raw_alpha

mod_rapi <- "
          IM =~ 1 * IM_m
          EM =~ 1 * EM_m
          lInt =~ 1 * Int

          DV ~ b1*IM + b2*EM + b3*lInt

          # Variance
          IM ~~ v1*IM
          EM ~~ v2*EM
          lInt ~~ v3*lInt
          
          # Error variance
          IM_m~~ ev_IM * IM_m
          EM_m ~~ ev_EM * EM_m
          Int ~~ ev_Int * Int
          
          # Error Constraints
          ev_IM == (1 - IM_rel) * v1 / IM_rel
          ev_EM == (1 - EM_rel) * v2 / EM_rel
          ev_Int == ev_IM * v2 + ev_EM * v1 + ev_IM * ev_EM
          
          # Covariance
          IM ~~ cov_12*EM
          IM ~~ cov_13*lInt
          EM ~~ cov_23*lInt
          
          # Disturbance
    			DV ~~ dist_y*DV
          var_y :=  dist_y + (b1^2 * v1 + b2^2 * v2 + b3^2 * v3 + 
                              2 * b1 * b2 * cov_12 + 
                              2 * b1 * b3 * cov_13 + 
                              2 * b2 * b3 * cov_23)
          
          beta1 := b1*sqrt(v1)/sqrt(var_y)
          beta2 := b2*sqrt(v2)/sqrt(var_y)
          beta3 := b3*sqrt(v1)*sqrt(v2)/sqrt(var_y)
          "

mod_rapi <- gsub("IM_rel", replacement = IM_rel, x = mod_rapi)
mod_rapi <- gsub("EM_rel", replacement = EM_rel, x = mod_rapi)

# Fit the model using upi()
beta1_rapi <- c()
beta2_rapi <- c()
beta3_rapi <- c()
se1_rapi <- c()
se2_rapi <- c()
se3_rapi <- c()
p1_rapi <- c()
p2_rapi <- c()
p3_rapi <- c()

for (dv in DVs) {
  mod_rapi_temp <- gsub("DV", dv, mod_rapi)  # Replace "DV" with the current dv in the loop
  fit_rapi <- sem(mod_rapi_temp, rapi_df)
  beta1_rapi <- c(beta1_rapi, coef(fit_rapi, type = "user")["beta1"])
  beta2_rapi <- c(beta2_rapi, coef(fit_rapi, type = "user")["beta2"])
  beta3_rapi <- c(beta3_rapi, coef(fit_rapi, type = "user")["beta3"])
  se1_rapi <- c(se1_rapi, sqrt(vcov(fit_rapi, type = "user")["beta1", "beta1"]))
  se2_rapi <- c(se2_rapi, sqrt(vcov(fit_rapi, type = "user")["beta2", "beta2"]))
  se3_rapi <- c(se3_rapi, sqrt(vcov(fit_rapi, type = "user")["beta3", "beta3"]))
}

beta1_rapi_avg <- floor(mean(beta1_rapi) * 100) / 100
beta2_rapi_avg <- floor(mean(beta2_rapi) * 100) / 100
beta3_rapi_avg <- floor(mean(beta3_rapi) * 100) / 100

se1_rapi_avg <- combine_se(beta1_rapi, se1_rapi)
se2_rapi_avg <- combine_se(beta2_rapi, se2_rapi)
se3_rapi_avg <- combine_se(beta3_rapi, se3_rapi)

p1_rapi_avg <- combine_p_value(beta1_rapi_avg, se1_rapi_avg)
p2_rapi_avg <- combine_p_value(beta2_rapi_avg, se2_rapi_avg)
p3_rapi_avg <- combine_p_value(beta3_rapi_avg, se3_rapi_avg)

# 2SPA
# fs_dat
mod_cfa <- '
          IM =~ IM1 + IM2 + IM3
          EM =~ EM1 + EM2 + EM3
'
# Obtain factor scores
fs_dat <- get_fs(final_df, model = mod_cfa, method = "Bartlett", std.lv = TRUE)
# Obtain factor product
fs_dat$fs_int <- fs_dat$fs_IM * fs_dat$fs_EM
# Centering the product variable
fs_dat$fs_int <- fs_dat$fs_int - mean(fs_dat$fs_int)
fs_dat$fs_int_se <- sqrt(1 * fs_dat$fs_IM_se^2 + 1 * fs_dat$fs_EM_se^2 + fs_dat$fs_IM_se^2*fs_dat$fs_EM_se^2)

# tspa model
mod_tspa <- "
            # latent variables (indicated by factor scores)
            IM =~ 1 * fs_IM
            EM =~ 1 * fs_EM
            lInt =~ 1 * fs_int

            # constrain the errors
            fs_IM ~~ rel_IM * fs_IM
            fs_EM ~~ rel_EM * fs_EM
            fs_int ~~ rel_int * fs_int
            
            # regressions
            DV ~ b1*IM + b2*EM + b3*lInt

            # Variance
            IM ~~ v1*IM
            EM ~~ v2*EM
            lInt ~~ v3*lInt
            
            # Covariance
            IM ~~ cov_12*EM
            IM ~~ cov_13*lInt
            EM ~~ cov_23*lInt
          
            # Disturbance
    			  DV ~~ dist_y*DV
            var_y :=  dist_y + (b1^2 * v1 + b2^2 * v2 + b3^2 * v3 + 
                              2 * b1 * b2 * cov_12 + 
                              2 * b1 * b3 * cov_13 + 
                              2 * b2 * b3 * cov_23)
              
            # Define Standardized Coefficients
            beta1 := b1*sqrt(v1)/sqrt(var_y)
            beta2 := b2*sqrt(v2)/sqrt(var_y)
            beta3 := b3*sqrt(v1)*sqrt(v2)/sqrt(var_y)"
  
mod_tspa <- gsub("rel_IM", replacement = fs_dat$fs_IM_se[1]^2, x = mod_tspa)
mod_tspa <- gsub("rel_EM", replacement = fs_dat$fs_EM_se[1]^2, x = mod_tspa)
mod_tspa <- gsub("rel_int", replacement = fs_dat$fs_int_se[1]^2, x = mod_tspa)

beta1_tspa <- c()
beta2_tspa <- c()
beta3_tspa <- c()
se1_tspa <- c()
se2_tspa <- c()
se3_tspa <- c()
p1_tspa <- c()
p2_tspa <- c()
p3_tspa <- c()

for (dv in DVs) {
  if ("DV" %in% names(fs_dat)) {
    fs_dat$DV <- NULL
  }
  fs_dat$DV <- PIRLS_Data[[dv]]
  fit_tspa <- sem(mod_tspa,
                  data = fs_dat)
  beta1_tspa <- c(beta1_tspa, coef(fit_tspa, type = "user")["beta1"])
  beta2_tspa <- c(beta2_tspa, coef(fit_tspa, type = "user")["beta2"])
  beta3_tspa <- c(beta3_tspa, coef(fit_tspa, type = "user")["beta3"])
  se1_tspa <- c(se1_tspa, sqrt(vcov(fit_tspa, type = "user")["beta1", "beta1"]))
  se2_tspa <- c(se2_tspa, sqrt(vcov(fit_tspa, type = "user")["beta2", "beta2"]))
  se3_tspa <- c(se3_tspa, sqrt(vcov(fit_tspa, type = "user")["beta3", "beta3"]))
}

beta1_tspa_avg <- floor(mean(beta1_tspa) * 100) / 100
beta2_tspa_avg <- floor(mean(beta2_tspa) * 100) / 100
beta3_tspa_avg <- floor(mean(beta3_tspa) * 100) / 100

se1_tspa_avg <- combine_se(beta1_tspa, se1_tspa)
se2_tspa_avg <- combine_se(beta2_tspa, se2_tspa)
se3_tspa_avg <- combine_se(beta3_tspa, se3_tspa)

p1_tspa_avg <- combine_p_value(beta1_tspa_avg, se1_tspa_avg)
p2_tspa_avg <- combine_p_value(beta2_tspa_avg, se2_tspa_avg)
p3_tspa_avg <- combine_p_value(beta3_tspa_avg, se3_tspa_avg)
```

```{r PIRL 2021 Table, echo=FALSE, message=FALSE, warning=FALSE}
# Helper Function
format_p_value <- function(p) {
  if (p < 0.001) {
    return("\\makebox[2cm][c]{$< .001^{***}$}")
  } else if (p < 0.01) {
    return(sprintf("\\makebox[2cm][c]{$%.3f^{**}$}", p))
  } else if (p < 0.05) {
    return(sprintf("\\makebox[2cm][c]{$%.3f^{*}$}", p))
  } else {
    return(sprintf("\\makebox[2cm][c]{%.3f}", p))
  }
}
# Create the data frame with LaTeX \makebox for width control
PIRL_result <- data.frame(
  Approach = c("\\makebox[4cm][c]{Matched-Pair UPI}", 
               "\\makebox[4cm][c]{RAPI}", 
               "\\makebox[4cm][c]{2S-PA-Int}"),
  Beta_1 = round(c(beta1_upi_avg, beta1_rapi_avg, beta1_tspa_avg), 3),
  SE_1 = round(c(se1_upi_avg, se1_rapi_avg, se1_tspa_avg), 3),
  p_1 = sapply(c(p1_upi_avg, p1_rapi_avg, p1_tspa_avg), format_p_value), 
  Beta_2 = round(c(beta2_upi_avg, beta2_rapi_avg, beta2_tspa_avg), 3),
  SE_2 = round(c(se2_upi_avg, se2_rapi_avg, se2_tspa_avg), 3),
  p_1 = sapply(c(p2_upi_avg, p2_rapi_avg, p2_tspa_avg), format_p_value), 
  Beta_3 = round(c(beta3_upi_avg, beta3_rapi_avg, beta3_tspa_avg), 3),
  SE_3 = round(c(se3_upi_avg, se3_rapi_avg, se3_tspa_avg), 3),
  p_1 = sapply(c(p3_upi_avg, p3_rapi_avg, p3_tspa_avg), format_p_value)
)

# Name the columns with LaTeX formatting
names(PIRL_result) <- c("Approach", 
                        "$\\hat{\\beta}_{IM}$", "$SE_{IM}$", "$\\textit{p}_{IM}$", 
                        "$\\hat{\\beta}_{EM}$", "$SE_{EM}$", "$\\textit{p}_{EM}$",
                        "$\\hat{\\beta}_{IM \\times EM}$", "$SE_{IM \\times EM}$", "$\\textit{p}_{IM \\times EM}$")

# Create the APA-style table
PIRL_table <- apa_table(
  PIRL_result,
  escape = FALSE,
  landscape = TRUE,
  caption = "Parameter Estimates of the Latent Interaction Effect with Three Methods.",
  align = rep("c", ncol(PIRL_result)),  # Center-align all columns
  note = "$\\textit{N}$ = 4,900. Matched-Pair UPI = matched-pair product unconstrained indicator; RAPI = reliability-adjusted product indicator; 2S-PA-Int = two-stage path analysis with interaction. $\\hat{\\beta}_{IM}$ and $\\hat{\\beta}_{EM}$ denoted the first-order effect of IM and EM on reading performance scores, and $\\hat{\\beta}_{IM \\times EM}$ was their latent interaction effect. SE represented the standard error of measurement and $\\textit{p}$ denoted the significance value. The results showed significant first-order and latent interaction effects using all three methods.")

PIRL_table
```

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{/Users/jimmy_z/R Projects/2SPA-Int/Paper/Qual 1 Paper Draft/PIRL_Plot.png}
\caption{Structural Model of Illustrative Example from Park (2011).}
\caption*{\textit{Note. The model includes two first-order latent variables, intrinsic motivation and extrinsic motivation, depicted as ellipses. Their latent interaction term was depicted as a filled black circle. The dependent variable was observed and rendered as a rectangle.}}
\end{figure}

# Discussion

Applied researchers often explore complex relationships between variables, such as interactions. However, classical regression models, which assume that variables are free from measurement error, have been shown to yield biased estimates. As a result, latent variable approaches within the SEM framework are gaining prominence. In this study, we reviewed and compared the performance of three latent interaction methods (matched-pair UPI, RAPI, and 2S-PA-Int) in estimating interaction effects on congeneric items with varying factor loadings and measurement errors. Additionally, the regression-based approach using observed indicators, MMR, was included as a reference method.

We extended the 2S-PA model by @laiTwostagePathAnalysis2022a to support latent interaction estimation, namely 2S-PA-Int. The primary distinction between matched-pair UPI, RAPI, and 2S-PA-Int lies in the formation of latent interaction term. Matched-pair UPI constructs the interaction term using multiple product indicators (PIs) generated from first-order indicators, making it a multiple-indicator method. In contrast, RAPI and 2S-PA-Int use composite scores and factor scores as single indicators (SIs) for the interaction term, respectively.

Our results demonstrated that the MMR approach, based on observed indicators, consistently yielded substantially downward biased estimates of interaction effect path coefficients across multiple conditions. This finding can be attributed to the method's inefficiency to properly account for measurement errors in the observed items. The underestimated coefficients are consistent with previous research, which has emphasized that measurement error might result in biased parameter estimates (Dunlap & Kemery, 1988; Evans, 1985). 

In contrast, the latent interaction methods were effective in producing unbiased estimates of interaction effects by accounting for measurement errors, as demonstrated in our simulation study. However, both RAPI and matched-pair UPI exhibited notably positive standardized bias (SB), suggesting a tendency to overestimate interaction effects when true effects were present. These findings aligned with previous research by Marsh et al. (2004), Hsiao et al. (2018), and Hsiao et al. (2021), which similarly reported overestimation of interaction effects using matched-pair UPI and RAPI, particularly when dealing with congeneric items or tau-equivalent items with varied error variances. 2S-PA-Int also showed a tendency to overestimate interaction effects, further emphasizing that latent interaction methods should be applied carefully and cautiously, especially when more conservative estimates are needed. In terms of accuracy in estimating interaction effects, 2S-PA-Int demonstrates comparability to other latent interaction methods and presents a more reliable alternative to MMR when estimating interaction effects using congeneric items with measurement error. 

One challenge in using latent variable modeling approaches for interaction effects is the risk of generating unstable estimates across replications, as reflected by the convergence rates and relative standard error (SE) estimates in the simulation results. In some cases, extreme SE estimates reaching values as high as 200 were observed, which is neither reasonable nor appropriate for coefficient interpretation and model comparison. This finding is consistent with previous research by Hsiao et al. (2021) and Ledgerwood & Shrout (2011), such that while latent interaction models improve accuracy by accounting for measurement error, they can also introduce increased variability in parameter estimates. Besides, Hsiao and Lai (2018) noted that constraining measurement errors for highly reliable variables may lead to over-adjustment of SE, particularly with small sample sizes. Our RMSE results supported this finding, such that latent interaction methods generally exhibited higher RMSE values compared to MMR when the sample size was 100. Consequently, a latent variable model that can simultaneously yield both accurate and stable estimation should be recommended. Although all three latent interaction methods in our simulation study showed unacceptable relative SE bias in some small sample size and low reliability conditions, 2S-PA-Int generally demonstrates comparable stability in estimating interaction effects. In addition, 2S-PA-Int among the latent interaction methods produced the lowest RMSE values that were nearly comparable to those of MMR, which further supports that it has potential of taking into account accuracy and variability of parameter estimation. 

With respect to coverage rates, RAPI showed notably higher coverage rates than matched-pair UPI and 2S-PA-Int, which can be partially attributed to its inflated SE estimation. While slightly lower, 2S-PA-Int also achieved acceptable coverage rates over 93%, suggesting its capacity for capturing true interaction effects reliably. The results imply that both RAPI and 2S-PA-Int possess sufficient capability of effectively detecting interaction effects across varied conditions. In contrast, matched-pair UPI is not consistently robust to small sample sizes and low reliability levels. The observation is aligned with Marsh et al. (2004), although it should be noted that Marsh et al. (2004) did not evaluate matched-pair UPI with fully congeneric items, which may partly explain its reduced ability to capture true effects under such conditions. By ignoring measurement error, MMR failed to show sufficient coverage rates across almost all conditions, indicating that in general it could not capture true interaction effects. One possible reason is that downward SE estimates of  MMR result in narrower confidence intervals, which increases the likelihood of missing true effects.

Revisiting Marsh's criteria for an effective latent interaction model, 2S-PA-Int stands out for its simplicity as a single-indicator method and its efficient use of information through factor scores based on all first-order indicators. Models burdened with excessive indicators often face convergence issues due to complex covariance structures, potentially resulting in non-identifiable models [@bollenStructuralEquationsLatent1989d]. Moreover, @byrneStructuralEquationModeling2016 points out that too many indicators can introduce redundancy, unnecessarily complicating the model and increasing the risk of estimation problems. Therefore, 2S-PA-Int emerges as a good alternative to matched-pair UPI in terms of simpler model and stable parameter estimation, especially with a large number of first-order indicators. Compared to RAPI, 2S-PA-Int also offers greater stability and accuracy in estimating interaction effects. Overall, latent interaction methods for composite scores are preferable to MMR when considering both precision and bias in the estimation of interaction effect, with 2S-PA-Int demonstrating the greatest potential among the methods.

While 2S-PA-Int demonstrated promising statistical properties in our simulation study, it is important to recognize several limitations in the limited scope of study design. First, given that the study focused exclusively on product indicator (PI) methods, distribution-analytic approaches such as the latent moderated structural equation (LMS; Klein & Moosbrugger, 2000) method, and other alternative methods, were not included. Previous research has shown that LMS tends to produce unbiased estimates of latent interaction effects with acceptable statistical power when applied to congeneric items with normal distributions (Hsiao et al., 2021; Cham et al., 2012). Future studies can incorporate more alternative methods of estimating latent interaction effects to expand the scope of study. 

Second, with regard to method application, we do not recommend the use of 2S-PA-Int for extreme cases where the sample size is less than 100 and item reliability falls below 0.7. Additionally, as Hsiao et al. (2018) noted, RAPI may be more practical for researchers working with secondary datasets, where only composite scores and their corresponding reliability indices (e.g., Cronbach’s alpha) are typically available. In such cases, when factor scores and their standard errors are not provided, researchers may be unable to compute factor scores, thereby limiting the feasibility of applying 2S-PA-Int. Furthermore, the present study focused on congeneric items that were continuous and normally distributed. However, much research has highlighted the frequent use of categorical data in psychological studies to assess qualitative dimensions of human behavior, attitudes, and traits (Brown, 2015; Kline, 2016). Despite the lack of evaluation of 2S-PA-Int with categorical items in this study, its ability to incorporate observation-specific standard errors of measurement suggests that it may be well-suited for estimating latent interaction effects with categorical data in future research (Lai et al., 2023).

Additionally, previous research on latent interaction effects has typically employed simplified designs with two latent predictors and a single interaction term, which may not adequately reflect the complexity of real-world scenarios that involve multiple interaction terms. Given the increasing prevalence of multilevel designs in educational, counseling, and organizational research (e.g., students nested within classrooms, patients within clinics, employees within companies), it is important to investigate the applicability of 2S-PA-Int in handling more complex data structures. Future research could explore how 2S-PA-Int performs in multilevel contexts, particularly under varying sample sizes and reliability levels, to assess its robustness and versatility in such advanced analytical frameworks.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
