---
title             : "Two-Stage Path Analysis with Interaction: A Good Alternative to Current Methods of Modeling Latent Interaction"
shorttitle        : "2S-PA-Int"

author: 
  - name          : "Gengrui (Jimmy) Zhang"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    email         : "gengruiz@email.com"
    role: # Contributorship roles (e.g., CRediT, https://credit.niso.org/)
      - "Conceptualization"
      - "Writing - Original Draft Preparation"
      - "Writing - Review & Editing"

affiliation:
  - id            : "1"
    institution   : "University of Southhern California"

abstract: |
  Modeling interaction effects within the latent variable modeling framework has become increasingly popular in psychological research as it facilitates the exploration of in-depth theory and complex data structure. Comprared to the extensitvely used regression-based approaches assuming error-free variables, the latent interaction approach is able to account for measurement error and produce estimates with less bias and more accurate standard error. In this study, we investigated three product indicator methods based on structural equation modeling (SEM): Matched-pair Unconstrained Product Indicator (UPI), Reliability-Adjusted Product Indicator (RAPI), and an extended model based on the two-stage path analysis (2S-PA) framework, namely 2S-PA-Int, by conducting a simulation study with 2,000 replications. The results showed that 2S-PA-Int produced consistently less standardized bias, acceptable relative SE bias and coverage rates, and lower RMSE values than matched-pair UPI and RAPI, particularly under the conditions of small sample size and low reliability. Generally, 2S-PA-Int showed promising statistical properties and simpler model specification, indicating that it could serve as a competitive alternative of existing methods. Future research directions of 2S-PA-Int were discussed.
  
  <!-- https://tinyurl.com/ybremelq -->
  
keywords          : "Latent interaction, UPI, RAPI, 2S-PA"

bibliography      : "r-references.bib"
notice            : "@kleinMaximumLikelihoodEstimation2000a"

floatsintext      : no
linenumbers       : yes
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
csl               : "`r system.file('rmd', 'apa7.csl', package = 'papaja')`"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library(haven)
library(dplyr)
library(tidyr)
library(stringr)
library(here)
library(tidyverse)
library(semTools)
library(magick)
r_refs("r-references.bib")

r_scripts <- list.files(here("R"), pattern = "\\.R$", full.names = TRUE)
lapply(r_scripts, source)
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

```{r message=FALSE, warning=FALSE}
sim <- readRDS(here("Sim_Data", "Match_07012024.rds"))
sim_results <- sim %>% 
  dplyr::select(-c(REPLICATIONS:WARNINGS)) %>%
  mutate(across(where(is.numeric), round, 4),
         cor_xm = ifelse(cor_xm == 0, "0", cor_xm))
sim_null <- sim_results %>% filter(gamma_xm == 0)
sim_alt <- sim_results %>% filter(gamma_xm != 0)
```

Social science research increasingly focuses on intricate dynamics of complex phenomena, such as nonlinear and moderation effects, rather than merely simple bivariate relationship. This shift reflects the multifaceted nature of the real world, which seldom conforms to straightforward patterns [@cartePursuitModerationNine2003b; @mackinnonHowWhomMediation2008a; @cunninghamModerationSportManagement2019a]. For instance, while earlier studies have established that exercise contributes to weight loss, there is a burgeoning interest in understanding and probing into the underlying mechanisms, such as optimal timing, specific target populations, and the contextual conditions that modulate the effectiveness of exercise in promoting weight loss. Investigations into moderation, or interaction effects, provide critical insights into these inquiries by examining how additional variables—or an ensemble of variables—shape the dynamics between the primary variables of interest.

A prevalent approach to modeling moderation is through regression analysis, specifically by incorporating an interaction term, $XZ$:
\begin{equation}
Y = b_{0} + b_{1}X + b_{2}Z + b_{3}XZ + \epsilon,
\end{equation}
where $b_{0}$ is the intercept, $b_{1}$ and $b_{2}$ are the regression coefficients for $X$ and $Z$ respectively, $b_{3}$ is the coefficient for the interaction term $XZ$, and $\epsilon$ is the error term. To maintain consistency with the naming convention used by @marshStructuralEquationModels2004a, we refer to main effects (i.e., non-interaction effects) as "first-order effects". Hence $X$ and $Z$ are first-order variables and $b_{1}$ and $b_{2}$ are first-order effects in this case. 

Classical regression model typically assumes that variables are measured without error, a premise that can lead to biased parameter estimates when measurement errors are present in empirical research [@bollenStructuralEquationsLatent1989d; @cohenAppliedMultipleRegression2003; @carrollMeasurementErrorNonlinear2006]. This bias may be particularly more inflated for interaction effects [@andersonComparisonBiasMean1996]. To mitigate this issue, researchers use latent variables that are inferred and measured by a set of observed indicators within the structural equation modeling (SEM) framework, which can control and accommodate measurement errors in observed indicators [@bollenLatentVariablesPsychology2002a]. For example, depression is widely measured and assessed using the Center for Epidemiologic Studies Depression (CES-D) scale consisting of 20 items [@radloffCESDScaleSelfreport1977b]. An expanding body of research has demonstrated that SEM-based moderation models reliably provide more accurate representations of the relationships among latent constructs [@muellerStructuralEquationModeling1997a; @steinmetzThreeApproachesEstimate2011a; @chamEstimatingLatentVariable2012a; @maslowskyEstimatingInterpretingLatent2015a]. 

The Two-Stage Path Analysis (2S-PA; Lai & Hsiao, 2022) method models pathway relationships among latent variables through the use of factor scores. Simulation studies have shown its ability to yield parameter estimates with reduced standard error bias, enhanced convergence rates, and improved management of Type I error, particularly in small sample contexts [@laiTwostagePathAnalysis2022a; @laiCorrectingUnreliabilityPartial2023]. Given its promising statistical property, simpler model specification, and easier implementation in widely used software, we extended the 2S-PA method to incorporate latent interaction estimation in this study, and named it 2S-PA-Int. We reviewed two widely used latent interaction models using the product indicator method: Unconstrained Product Indicator with Matched Pairs (Matched-Pair UPI; Marsh et al., 2004) and Reliability-Adjusted Product Indicator (RAPI; Hsiao et al., 2018). Then we conducted a Monte Carlo simulation study to compare their performance with 2S-PA-Int. To proceed, we first introduced a classical model of latent interaction, and then presented UPI, RAPI, and 2S-PA-Int with technical details.

## A Classical Model of Latent Interaction

@kennyEstimatingNonlinearInteractive1984a introduced a seminal structural model for estimating latent interaction effects, particularly in scenarios involving two latent predictors and their interaction term:
\begin{equation}
y = \alpha + \gamma_{x}\xi_{x} + \gamma_{m}\xi_{m} + \gamma_{xm}\xi_{x}\xi_{m} + \zeta,
\end{equation} 
where $\alpha$ is the constant intercept, $\xi_{x}$ and $\xi_{m}$ are the first-order latent predictors, and the product $\xi_{x}\xi_{m}$ defines the interaction effect. Note that $\xi_{x}$ and $\xi_{m}$ are allowed to correlate with each other. The disturbance term $\zeta$ in the model is assumed to follow a normal distribution, $\zeta \sim N(0, \psi)$, where $\psi$ denotes the variance of $\zeta$, accounting for unobserved factors that influence the dependent variable.  The coefficients $\gamma_{x}$ and $\gamma_{m}$ capture the first-order effects of the latent predictors, while $\gamma_{xm}$ measures the latent interaction effect. The dependent variable $y$ in this model can be either an observed variable or a latent construct, allowing for flexibility in its application.

The measurement model for the first-order latent predictors, such as $\xi_{x}$, can be articulated by the following confirmatory factor analysis (CFA) framework:
\begin{equation}
\mathbf{x} = \boldsymbol{\tau_{x}} + \boldsymbol{\lambda_{x}}\xi_{x} + \boldsymbol{\delta_{x}},
\end{equation}
wherein, for each indicator $i = 1, \ 2, \ ..., \ p$ associated with the latent predictor $\xi_{x}$, $\mathbf{x}$ denotes a $p \times 1$ vector of observed first-order indicators (i.e., the indicators of $\xi_{x}$). The term $\boldsymbol{\tau_{x}}$ represents a $p \times 1$ vector of constant intercepts, while $\boldsymbol{\lambda}{x}$ is a $p \times 1$ vector of factor loadings, which capture the strength of the relationship between the latent variable $\xi{x}$ and each of its indicators. The vector $\boldsymbol{\delta_{x}}$ represents the $p \times 1$ vector of measurement errors associated with these indicators. Each measurement error $\delta_{x_{i}}$ is normally distributed with a mean of zero and a variance of $\theta_{x_{i}}$. Under the assumption of local independence, which posits that the first-order indicators are uncorrelated with one another when they are indicators of the same latent variable, the variance-covariance matrix of all the indicators' measurement errors is a diagonal matrix, denoted as $\mathbf{\Theta_{\delta_{x}}} = \text{diag}(\theta_{x_{1}}, \theta_{x_{2}}, ..., \theta_{x_{p}})$. This measurement model, along with its associated parameters, is similarly applicable to the latent predictor $\xi_{m}$, ensuring consistency in the modeling of both latent variables.

Kenny and Judd's original formulation of model omitted the intercept $\alpha$, a point subsequently addressed by @Jreskog1996NonlinearSE, who revised the model under a set of assumptions. The revised latent interaction model is grounded in three primary assumptions related to multivariate normal distribution and independence: (1) The measurement errors of first-order indicators, the first-order latent predictors, and the disturbance term in the structural model are multivariate normal, uncorrelated, and independent to each other (i.e., $Corr[\delta, \xi] = 0$; $Corr[\zeta, \xi] = 0$; $Corr[\delta, \zeta] = 0$, where $Corr$ denotes the correlation index); (2) All measurement errors are mutually independent and uncorrelated to each other (i.e., $Corr[\delta_{i}, \delta_{i'}] = 0$ for $i \neq i'$); (3) The correlation between the first-order latent predictors, $Corr[\xi_{x}, \xi_{m}]$, is assumed to be non-zero and is freely estimated. This approach accounts for the fact that the product term $\xi_{x}\xi_{m}$ may exhibit a non-normal distribution even when $\xi_{x}$ and $\xi_{m}$ are themselves normally distributed with means of 0 [@Jreskog1996NonlinearSE].

@alginaNoteEstimatingJoreskogYang2001 refined Jöreskog and Yang's (1996) model by introducing the use of mean-centered first-order indicators (e.g., $x_{i} - \mu_{x_{i}}$, where $\mu_{x_{i}}$ represents the mean of $x_{i}$) to construct product indicators (PI) that capture the latent interaction term. This enhancement significantly improves the model by rendering parameter estimates more interpretable, facilitating a higher rate of model convergence, and reducing estimation bias [@alginaNoteEstimatingJoreskogYang2001; @moulderComparisonMethodsEstimating2002a; @marshStructuralEquationModels2004a]. Moreover, the practice of mean-centering first-order indicators effectively mitigates the problem of multicollinearity, thereby more distinctly delineating the contributions of the first-order latent variables and their interactions, as highlighted by @schoemannTestingInterpretingLatent2021.

## Unconstrained Product Indicator (UPI)

While @alginaNoteEstimatingJoreskogYang2001 significantly improved the model, their approach required complicated nonlinear constraints on parameters of PIs and the interaction term. Constraints in SEM are predefined conditions or restrictions applied to model parameters to ensure model identifiability, theoretical consistency, and interpretability [@klinePrinciplesPracticeStructural2016]. Consider, for example, that $x_{2}$ and $m_{2}$ are two first-order indicators of respective latent predictors $\xi_{x}$ and $\xi_{m}$, with their corresponding PI formed as $x_{2}m_{2}$. Then $x_{2}m_{2}$ can be decomposed using the measurement model of $x_{2}$ and $m_{2}$:

\begin{equation}
x_{2}m_{2}= (\lambda_{x_{2}}\xi_{x} + \delta_{x_{2}})(\lambda_{m_{2}}\xi_{m} + \delta_{m_{2}}),
\end{equation}
where $\lambda$ is the factor loading, $\xi$ is the first-order latent variable, and $\delta$ is the error term of first-order indicators. After expanding the equation, it can be shown that the factor loading of this formed PI is a function of first-order indicators' factor loadings, such that $\lambda_{x_{2}m_{2}} = \lambda_{x_{2}}\lambda_{m_{2}}$. Similarly, the error term can be derived as a function of parameters from first-order indicators: $\delta_{x_{2}m_{2}} = \lambda_{x_{2}}\xi_{x}\delta_{m_{2}} + \lambda_{m_{2}}\xi_{m}\delta_{x_{2}} + \delta_{x_{2}}\delta_{m_{2}}$. As the number of first-order indicators increases, the model specification becomes overwhelmingly cumbersome due to the resultant nonlinear constraints, which can pose challenges to model convergence.

@marshStructuralEquationModels2004a explored methods to eliminate these complex constraints and introduced the innovative Unconstrained Product Indicator (UPI) approach, which simplifies model specification and decreases the likelihood of convergence issues. The structural model of UPI is identical to the model presented in equation (2), with the exception of omitting the intercept $\alpha$. To illustrate this approach, consider a measurement model where the latent variables $\xi_{x}$ and $\xi_{m}$ are each associated with three indicators:

\begin{align}
    \begin{bmatrix}
        x_{1} \\
        x_{2} \\ 
        x_{3}
    \end{bmatrix} =
    \begin{bmatrix}
        \tau_{x_{1}} \\
        \tau_{x_{2}} \\ 
        \tau_{x_{3}}
    \end{bmatrix} +
    \begin{bmatrix}
        \lambda_{x_{1}} \\
        \lambda_{x_{2}} \\ 
        \lambda_{x_{3}}
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{x} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{x_{1}} \\
        \delta_{x_{2}} \\ 
        \delta_{x_{3}}
    \end{bmatrix},
\end{align}

\begin{align}
    \begin{bmatrix}
        m_{1} \\
        m_{2} \\ 
        m_{3}
    \end{bmatrix} =
    \begin{bmatrix}
        \tau_{m_{1}} \\
        \tau_{m_{2}} \\ 
        \tau_{m_{3}}
    \end{bmatrix} +
    \begin{bmatrix}
        \lambda_{m_{1}} \\
        \lambda_{m_{2}} \\ 
        \lambda_{m_{3}}
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{m_{1}} \\
        \delta_{m_{2}} \\ 
        \delta_{m_{3}}
    \end{bmatrix}
\end{align} 

@marshStructuralEquationModels2004a introduced two methods for specifying UPI: the all-pair UPI and the matched-pair UPI. In the all-pair UPI model, the latent interaction term is represented by all possible pairings of the first-order indicators of $\xi_{x}$ and $\xi_{m}$:

\begin{align}
    \begin{bmatrix}
        x_{1}m_{1} \\
        x_{1}m_{2} \\
        x_{1}m_{3} \\ 
        x_{2}m_{1} \\
        ... \\
        x_{3}m_{3}
    \end{bmatrix} = 
    \begin{bmatrix}
        \tau_{x_{1}m_{1}} \\
        \tau_{x_{1}m_{2}} \\ 
        \tau_{x_{1}m_{3}} \\ 
        \tau_{x_{2}m_{1}} \\ 
        ...\\
        \tau_{x_{3}m_{3}} 
    \end{bmatrix} +
    \begin{bmatrix}
        \lambda_{x_{1}m_{1}} \\
        \lambda_{x_{1}m_{2}} \\ 
        \lambda_{x_{1}m_{3}} \\ 
        \lambda_{x_{2}m_{1}} \\ 
        ...\\
        \lambda_{x_{3}m_{3}}
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{x}\xi_{m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{x_{1}m_{1}} \\
        \delta_{x_{1}m_{2}} \\ 
        \delta_{x_{1}m_{3}} \\
        \delta_{x_{2}m_{1}} \\
        ... \\
        \delta_{x_{3}m_{3}}
    \end{bmatrix},
\end{align}
where each PI is derived from multiplying two corresponding mean-centered first-order indicators, one from $\xi_{x}$ and the other from $\xi_{m}$ (e.g., the PI $x_{1}m_{1}$ is formed by the product of $x_{1}$ and $m_{1}$). The coefficients ${\tau_{x_{i}m_{i}}}$, ${\lambda_{x_{i}m_{i}}}$ and ${\delta_{x_{i}m_{i}}}$ are freely estimated as intercepts, factor loadings and measurement errors, respectively. The total number of PI is the multiplicative product of the number of first-order indicators for each latent predictor. In this case, nine unique PIs are formed ($3 \times 3 = 9$). 

Regarding the matched-pair UPI, the indicators are matched to create PIs:

\begin{align}
    \begin{bmatrix}
        x_{1}m_{1} \\
        x_{2}m_{2} \\
        x_{3}m_{3}
    \end{bmatrix} =
    \begin{bmatrix}
        \tau_{x_{1}m_{1}} \\
        \tau_{x_{2}m_{2}} \\ 
        \tau_{x_{3}m_{3}}
    \end{bmatrix} + 
    \begin{bmatrix}
        \lambda_{x_{1}m_{1}} \\
        \lambda_{x_{2}m_{2}} \\ 
        \lambda_{x_{3}m_{3}} 
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{x}\xi_{m} \\
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{x_{1}m_{1}} \\
        \delta_{x_{2}m_{2}} \\ 
        \delta_{x_{3}m_{3}}
    \end{bmatrix}
\end{align}

This alternative formulation leads to a significantly reduced number of PIs due to its simplicity. @marshStructuralEquationModels2004a argued that the matched-pair UPI is preferable based on two key criteria: (1) It leverages all available information by utilizing every first-order indicator, and (2) It avoids redundancy by ensuring that no first-order indicator is used more than once. Consequently, the matched-pair UPI method is recommended for its simplicity and effectiveness. Moreover, @marshStructuralEquationModels2004a demonstrated that the matched-pair UPI approach performs comparably to the all-pair model, exhibiting low bias and robustness to non-normal data. However, the matched-pair model is generally favored due to its greater simplicity and efficiency.

Since the mean of $\xi_{x}\xi_{m}$ may not equal to 0 even though $\xi_{x}$ and $\xi_{m}$ are assumed to have 0 means, @marshStructuralEquationModels2004a included a mean structure in their UPI model: $\mathbf{\kappa} = (0,\ 0,\ Cov[\xi_{x}, \xi_{m}])^T$, where $\mathbf{\kappa}$ should be the means of the three latent variables (see Algina & Boulder [2001] for more details). This adjustment ensures that the model accurately reflects the statistical relations between the first-order latent variables and their interaction term. @linStructuralEquationModels2010b further simplified the model by proposing a Double Mean Centering (DMC) strategy, wherein PIs composed of paired mean-centered first-order indicators are mean-centered again (e.g., $x_{i}m_{i} - \mu_{x_{i}m_{i}}$). DMC eliminates the need for including a mean structure in the UPI model and has been shown to perform well in parameter estimation, even when the normality assumption is violated. Consequently, we employed the UPI method with DMC in this study.

Although UPI with DMC has simpler model specification and better performance of parameter estimation compared to the classical model, an arbitrariness-complexity dilemma between the all-pair and the matched-pair methods remains unresolved [@foldnesChoiceProductIndicators2014]. Consider a scenario involving two complex psychological constructs as latent predictors, each requiring over 10 indicators to adequately capture the theoretical constructs. The all-pair UPI method could result in a latent interaction term indicated by hundreds of PIs. While having a large number of items can enhance the representation of latent constructs and theoretically increase the statistical power for detecting subtle effects, it also tends to create a cumbersome model. This complexity can negatively affect interpretability, escalate computational demands, and lead to overfitting. On the other hand, the matched-pair UPI strategy simplifies the model by reducing the number of necessary PIs but introduces the challenge of PI selection, particularly when researchers must handle unbalanced numbers of first-order indicators. For unbalanced indicators, researchers must decide how to properly form PIs, as multiple solutions exist. They might aggregate several observed indicators into fewer parcels [@jackmanEstimatingLatentVariable2011a] or prioritize items with higher reliability for PI formation [@wuComparisonStrategiesForming2013]. However, there is no consensus on the optimal strategy for forming matched pairs. The considerable arbitrariness across different approaches introduces uncertainty in selecting the best strategy and complicates the decision-making process in model specification. To address this issue, @wuComparisonStrategiesForming2013 investigated two solutions in which researchers could form PIs by using highly reliable first-order indicators (i.e., items with higher factor loadings) while ignoring those with low reliability, or by matching parcels of the larger group of first-order indicators with indicators of the smaller group. They recommended to form PIs in accordance with the order of item reliability, emphasizing the importance of leveraging the most reliable indicators to enhance model performance.

## Reliability Adjusted Product Indicator (RAPI)

The RAPI method, introduced by @hsiaoEvaluationTwoMethods2018a, also involves forming PIs, but it does so by using composite scores (either sum or mean scores) of multiple observed items. Specifically, this approach aggregates all first-order indicators into single indicators (SIs) to indicate first-order latent variables, and multiplies the first-order PIs to form the SI to indicate the latent interaction term. Consequently, the resulting PI is itself an SI. This method effectively circumvents the issue of arbitrariness in indicator selection while using all information without redundancy. RAPI adjusts for measurement error in composite scores by constraining error variances of SIs, thereby ensuring that parameter estimates are less biased. The model can be succinctly represented as follows:
\begin{align}
    \begin{bmatrix}
        x_{comp} \\
        m_{comp} \\
        x_{comp} \cdot m_{comp}
    \end{bmatrix} = 
    \begin{bmatrix}
        \tau_{x_{comp}} \\
        \tau_{m_{comp}} \\ 
        \tau_{x_{comp} \cdot m_{comp}} 
    \end{bmatrix} + 
    \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\ 
        0 & 0 & 1 
    \end{bmatrix}
    \begin{bmatrix}
        \xi_{x} \\  
        \xi_{m} \\ 
        \xi_{x}\xi_{m}
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{x_{comp}} \\
        \delta_{m_{comp}} \\ 
        \delta_{x_{comp} \cdot m_{comp}}
    \end{bmatrix},
\end{align}
where $x_{comp}$ and $m_{comp}$ are the composite scores formed by their corresponding first-order indicators, and $x_{comp} \cdot m_{comp}$ is the formed PI indicating the latent interaction term. These composite scores serve as SIs for their respective latent variables, with factor loadings uniformly constrained to $1$ for model identification. The measurement errors are represented by $\mathbf{\delta}$s.

A key characteristic of the RAPI method is its ability to accommodate measurement error in first-order indicators through the incorporation of error-variance constraints, which are calculated based on composite reliability. While composite reliability estimates for these error-variance constraints can be obtained using various methods, @hsiaoEvaluationTwoMethods2018a summarized and compared four normally used estimators for composite reliability: Cronbach's $\alpha$ [@cronbachCoefficientAlphaInternal1951], $\omega$ [@mcdonaldTheoreticalFoundationsPrincipal1970; @raykovEstimationCompositeReliability1997], the greatest lower bound reliability [@tenbergeGreatestLowerBound2004], and Coefficient H [@hancockReliabilityAradoxAssessing2011]. Suppose that $\rho_{xx'}$ denotes the estimated reliability index, the error variance of composite scores can be shown as a function of the reliability index: 
\begin{equation}
\hat{\sigma}^2_{\delta_{x}} = (1 - \rho_{xx'})\hat{\sigma}^2_{{x}},
\end{equation}
where $\hat{\sigma}^2_{\delta_{x}}$ represents the estimated error variance and $\hat{\sigma}^2_{{x}}$ represents the estimated total variance of the indicator. This formula can be linearly transformed to demonstrate the relationship between the error variances and the latent predictor in terms of reliability: $\hat{\sigma}_{\delta_{x}}^2 = [(1 - \rho_{xx'})/{\rho_{xx'}}]\hat{\sigma}^2_{\xi_{x}}$, where $\hat{\sigma}^2_{\xi_{x}}$ represents the estimated latent variance of $\xi_{x}$ and $\hat{\sigma}^2_{{x}} = {\hat{\sigma}^2_{\xi_{x}} + \hat{\sigma}^2_{\delta_{x}}}$ as derived from classical test theory [@lordStatisticalTheoriesMental1968]. Thus, under the assumption of independently and identically distributed measurement error, the equation for the error-variance constraint of the interaction term $\xi_{x}\xi_{m}$ can be derived as:
\begin{equation}
\begin{aligned}
\hat{\sigma}^2_{\delta_{xm}} =\; & \rho_{xx'}\hat{\sigma}^2_{{x}}(1 - \rho_{mm'}\hat{\sigma}^2_{{m}})\; + \\&
                        \rho_{mm'}\hat{\sigma}^2_{{m}}(1-\rho_{xx'})\hat{\sigma}^2_{{x}}\; + \\&
                        (1 - \rho_{xx'})\hat{\sigma}^2_{{x}}(1 - \rho_{mm'})\hat{\sigma}^2_{{m}}. 
\end{aligned}
\end{equation}
More technical details are available in Appendix A of @hsiaoEvaluationTwoMethods2018a.

The use of composite scores as SIs evidently simplifies model specification, as the total number of PIs directly corresponds to the number of interaction terms. By accounting for measurement error, RAPI is expected to yield less biased estimates of interaction effects and exhibit enhanced statistical power. However, the method's effectiveness is contingent upon accurate estimation of reliability measures. Inaccurate reliability estimates, which form the basis for error constraints, can result in biased outcomes. Despite its manageable model complexity and ease of implementation, @hsiaoModelingMeasurementErrors2021 demonstrated that RAPI may produce non-positive definite matrices due to negative error variances and inflated interaction effect estimates, under conditions of low reliability (e.g., r = .70) and small sample size (e.g., N = 100). This suggests that RAPI may generate unstable interaction estimates under such conditions, highlighting the importance of carefully considering reliability and sample size when applying this method.

## Two-stage Path Analysis with Interaction (2S-PA-Int)

The 2S-PA method, as proposed by @laiTwostagePathAnalysis2022a, is an alternative approach to addressing measurement error within the context of multiple congeneric items (i.e., items with unique factor loadings and error variances; Jöreskog, 1971) by incorporating reliability adjustment. While it shares similarities with the RAPI method, 2S-PA uses factor scores as single indicators (SIs) for latent predictors. A key advancement of the 2S-PA approach is its capacity to assign observation-specific estimated reliability, thereby extending its applicability to ordered categorical items and accommodating distributions that deviate from normality [@laiTwostagePathAnalysis2022a; @laiCorrectingUnreliabilityPartial2023]. Moreover, conventional SEM models typically estimate measurement and structural models simultaneously, which necessitates an adequate sample size to achieve satisfactory convergence rates [@klinePrinciplesPracticeStructural2016; @kyriazosAppliedPsychometricsSample2018]. To address this potential issue, 2S-PA separates the step of specifying the measurement model from estimating the structural model, therefore alleviating computational burden and improving stability of parameter estimation. 

At the first stage of 2SPA, researchers obtain factor scores using first-order indicators for each participant $j$ for $j = 1, 2, ..., n$. Next, parallel to RAPI, the factor scores of latent predictors are multiplied to construct a PI for the interaction term $\xi_{x_{j}}\xi_{m_{j}}$:

\begin{align}
    \begin{bmatrix}
        \tilde{x}_{j} \\ 
        \tilde{m}_{j} \\
        \widetilde{xm}_{j} 
    \end{bmatrix} = 
    \begin{bmatrix}
        \tau_{\tilde{x}_{j}} \\
        \tau_{\tilde{m}_{j}} \\ 
        \tau_{\widetilde{xm}_{j}}
    \end{bmatrix} + 
    \begin{bmatrix}
        \lambda_{\tilde{x}_{j}} & 0 & 0 \\
        0 & \lambda_{\tilde{m}_{j}} & 0 \\ 
        0 & 0 & \lambda_{\widetilde{xm}_{j}} 
    \end{bmatrix} 
    \begin{bmatrix}
        \xi_{x_{j}} \\  
        \xi_{m_{j}} \\
        \xi_{x_{j}}\xi_{m_{j}}
    \end{bmatrix} +
    \begin{bmatrix}
        \delta_{\tilde{x}_{j}} \\
        \delta_{\tilde{m}_{j}} \\ 
        \delta_{\widetilde{xm}_{j}}
    \end{bmatrix},
\end{align}
wherein the factor scores $\tilde{x}_{j}$, $\tilde{m}_{j}$ and the PI $\widetilde{xm}_{j}$ are SIs of the respective latent variables. The intercepts, factor loadings, and error variances are all model parameters to be freely estimated. 

Researchers have several methods available for calculating factor scores (e.g., regression factor scores, expected-a-posterior factor scores), as reviewed in @estabrookComparisonFactorScore2013. In this study, We used Bartlett factor scores that are adjusted to have the same units as latent variables and constrained their factor loadings to 1 for model identification (i.e., $\lambda_{\tilde{x}_{j}} = \lambda_{\tilde{m}_{j}} = \lambda_{\widetilde{xm}_{j}} = 1$), as shown in @devliegerHypothesisTestingUsing2016 and @laiCorrectingUnreliabilityPartial2023.

Given that the focus of the current study is on continuous variables, and the first-order indicators of $\xi_{x_{j}}$ and $\xi_{m_{j}}$ are assumed to be normally distributed, the corresponding error variances are constant across all observations. The error variance constraints for factor scores are $\hat{\sigma}_{\tilde{x}_{j}}^2$, where $\hat{\sigma}_{\tilde{x}_{j}}$ is the estimated standard error of measurement of the factor score $\tilde{x}$ for the person $j$. The error-variance constraint for the interaction term is defined similarly as equation (11). In essence, the RAPI method is a special case of 2SPA where the composite scores are used for continuous items [@laiTwostagePathAnalysis2022a].

In this paper, we investigated whether the 2S-PA-Int approach is a reliable alternative to existing methods for estimating latent interaction effects, for its simplicity in model complexity and clarity in model specification. @laiTwostagePathAnalysis2022a demonstrated that 2S-PA provides robust and precise estimates with less SE bias, lower Type I error rate, and higher convergence rates in conditions of small sample size and low reliability. Therefore, we anticipated that the 2S-PA-Int method would inherit these advantages and exhibit strong performance in the estimation of latent interaction effects.

# Method

## Simulation Design

Adapted from @hsiaoModelingMeasurementErrors2021, the current simulation study aimed to compare the performance of moderated multiple regression (MMR), matched-pair UPI, RAPI, and 2S-PA-Int in estimating latent interaction effects for continuous congeneric items. We examined the bias and variance of interaction estimates by the four methods across various levels of sample size, reliability, and correlation between first-order latent variables. 

The population data was generated based on the model below with predefined parameter values:
\begin{equation}
\begin{gathered}
x_{i} =  \tau_{x_{i}} + \lambda_{x_{i}}\xi_{x} + \delta_{x_{i}};\\
m_{i} =  \tau_{m_{i}} + \lambda_{m_{i}}\xi_{m} + \delta_{m_{i}};\\
y =  \tau_{y} + \gamma_{x}\xi_{x} + \gamma_{m}\xi_{m} + \gamma_{xm}\xi_{x}\xi_{m} + \zeta,
\end{gathered}
\end{equation}
where the path coefficients of two latent predictors (i.e., $\gamma_{x}$ and $\gamma_{m}$) and their interaction term (i.e., $\gamma_{xm}$) were all set to 0.3 for the structural model. The first-order latent predictors $\xi_{x}$ and $\xi_{m}$ were simulated from standard normal distributions with means of 0 and variances fixed at 1, each indicated by three items (i.e., $\xi_{x}$ indicated by [$x_{1}$, $x_{2}$, $x_{3}$]; $\xi_{m}$ indicated by [$m_{1}$, $m_{2}$, $m_{3}$]). The first-order indicators and the dependent variable $y$ were all observed continuous variables with normally distributed error. Consequently, $\delta_{x_{i}}$, $\delta_{m_{i}}$ and $\zeta$ were assumed to have multivariate normal distributions and to be mutually independent. $\tau_{x_{i}}$, $\tau_{m_{i}}$, and $\tau_{y}$ were their corresponding constant intercepts and assumed to be 0. The first-order indicators were mean-centered for the four methods.

Drawing from @joreskogStatisticalAnalysisSets1971, congeneric tests were defined as a set of observed items measuring a latent construct with different factor loadings and unique error terms. These error terms were assumed to be uncorrelated with each other and with the latent construct, thereby representing random measurement error inherent to each item. To align with this concept, we manipulated the factor loadings and error variances for first-order indicators to create sets of congeneric items in the measurement model. Specifically, the first, second, and third indicators were set to fixed values of 1.0, 0.9, and 0.75 for both first-order latent variables (i.e., $\lambda_{x_{1}} = \lambda_{m_{1}} = 1.0$, $\lambda_{x_{2}} = \lambda_{m_{2}} = 0.9$, $\lambda_{x_{3}} = \lambda_{m_{3}} = 0.75$). According to equation (11), the error variance of the interaction term was a function of first-order indicators' reliability, suggesting that the interaction effect could be influenced by the level of measurement error. Therefore, we explored how each method performed under three different reliability conditions: 0.70, 0.80, and 0.90, which corresponded to three levels of error variances. Assuming that an appropriate reliability estimate had been obtained, the total error variance would be $[3.01, \ 1.76, \ 0.78]$ when $[\lambda_{x_{1}}, \ \lambda_{x_{2}}, \ \lambda_{x_{3}}] = [\lambda_{m_{1}}, \ \lambda_{m_{2}}, \ \lambda_{m_{3}}] = [1, \ 0.9, \ 0.75]$, as the reliability was varied at .70, .80, and .90 respectively. For each level, We systematically manipulated error variance proportions for each indicator, consistent with Hsiao et al. (2021).: 44$\%$ of the total error variance for the first indicator, 33$\%$ for the second, and 23$\%$ for the third. Taking the condition of $\rho = .70$ as an example, the error variances of three indicators would be manipulated as $1.32, \ 0.99, \ 0.69$. 

For MMR, relying solely on observed indicators, the structural equation followed equation (1), where $X$ and $Z$ were the sum scores of mean-centered first-order indicators. The latent interaction models involved more complex technical details in model specification. Following the suggestion of using matched-pair UPI by Marsh et al. (2004), we included the model in which $\xi_{x}\xi_{m}$ was indicated by three pairs of PIs: $x_{1}m_{1}$, $x_{2}m_{2}$, and $x_{3}m_{3}$. For the RAPI and 2SPA methods, $\xi_{x}\xi_{m}$ was loaded by a single PI. Specifically, RAPI uses PI of the mean scores of first-order indicators, whereas 2S-PA-Int uses PI of the Bartlett factor scores. To reduce the problem of multicollinearity between first-order latent predictors and the interaction term, the DMC strategy was applied to all the methods.

The methodological literature on latent interaction models exhibited a range of researcher-selected sample sizes from 20 to 5,000 [@chinPartialLeastSquares2003; @linStructuralEquationModels2010b; @chamEstimatingLatentVariable2012a], with common selections ranging from 100 to 500. Consequently, we chose N = 100, 250, and 500 to represent small, medium, and large sample sizes, respectively. As for the correlation between first-order latent predictors, we followed the study design in @hsiaoModelingMeasurementErrors2021 and pre-specified three population correlations $Corr[{\xi_{x},\xi_{m}}]$ (0, 0.3, 0.6) as zero to large correlation. Given that the variances of $y$ (i.e., $\sigma_{y}^2$), $\sigma_{\xi_{x}}^2$, and $\sigma_{\xi_{x}}^2$ were all set to 1, $\psi$ could be computed as $1 - R^2$ in which $R^2 = \gamma_{x}^2 + \gamma_{m}^2 + 2\gamma_{x}\gamma_{m}Corr[{\xi_{x},\xi_{m}}] + \gamma_{xm}^2(1 + Corr[{\xi_{x},\xi_{m}}]^2)$. For instance, $\psi = 1 - (0.3^2 + 0.3^2 + 2\times0.3\times0.3\times0 + 0.3^2\times(1 + 0)^2) = 0.73$ for $Corr[{\xi_{x},\xi_{m}}] = 0$. Similarly, $\psi$ = 0.668 and 0.590 for $Corr[{\xi_{x},\xi_{m}}]$ = 0.3 and 0.6, respectively.

In summary, our study implemented a $3 \times 3 \times 3$ factorial design, accommodating variations across three sample sizes, three levels of correlation between first-order latent predictors, and three levels of reliability.

## Evaluation Criteria

We chose widely used evaluation criteria that were summarized across 2,000 replications to evaluate the accuracy and precision of the interaction effect estimates ($\gamma_{xm}$) of the four methods. To facilitate the interpretation of path coefficients, we obtained and evaluated standardized estimates of $\gamma_{x}$, $\gamma_{m}$ and $\gamma_{xm}$.

### Raw Bias and Standardized Bias
Raw bias (RB) refers to the difference between the estimated and the true parameter value, while standardized bias (SB) normalizes RB by the standard error of the parameter estimates. This adjustment provides a standardized measure that allows for the comparison of bias across different scales or units of measurement. Given that SB reflected how far an estimate was from its true value in standard error units, it was expected to handle comparisons of models with various parameters (e.g., factor loadings, path coefficients).

In the current study, SB was defined as:

\begin{equation}
SB = \frac{RB(\gamma_{xm})}{SE_{\gamma_{xm}}},
\end{equation}

\begin{equation}
RB(\gamma_{xm}) = R^{-1}\Sigma^{R}_{r = 1}(\hat{\gamma}_{xm_{r}} - \gamma_{xm}),
\end{equation}
where R = 2,000 was the total number of replication cycles. $\hat{\gamma}_{xm_{r}}$ was the estimated interaction effect in each replication cycle $r$ and $\gamma_{xm}$ was the population parameter set at 0.3. $RB(\gamma_{xm})$ was the averaged deviation $\hat{\gamma}_{xm}$ from the population parameter, and $SE_{\gamma_{xm}}$ represented the empirical standard error of $\hat{\gamma}_{xm}$ across replications. @collinsComparisonInclusiveRestrictive2001 suggested that an absolute value of SB $\le 0.40$ would be considered acceptable for each replication condition.

### Coverage Rate
The coverage rate of a 95$\%$ confidence interval (CI) served as a critical metric for evaluating inferences of the interaction effect. It was defined as the percentage of replications in which the Wald confidence interval captured the true interaction effect $\gamma_{xm}$. Low coverage rate indicates that the confidence intervals often miss the true interaction effect, suggesting potential issues with the latent interaction method. A coverage rate larger than $91\%$ was considered acceptable [@muthenHowUseMonte2002].

### Robust Relative Standard Error Bias
The relative standard error (SE) bias was used to evaluate the precision of $\hat{\gamma}_{xm}$. This criterion compared the empirical standard deviation of $\hat{\gamma}_{xm}$ with the sample-estimated SE across replications:

\begin{equation}
Relative\ SE\ Bias = \frac{R^{-1}\Sigma^{R}_{r = 1}(\widehat{SE_{r}} - SD)}{SD},
\end{equation}
where $\widehat{SE}_{r}$ was the estimated standard error of $\hat{\gamma}_{xm}$ in replication cycle $r$ and $SD$ was the empirical standard deviation obtained from all replications. With $SD$ being used as a reference variability measure of $\hat{\gamma}_{xm}$, smaller relative SE bias meant the estimated standard errors were closer to the referenced variability, and the uncertainty of $\hat{\gamma}_{xm}$ across replications was more accurately measured in each simulation condition. Absolute values of relative SE bias $\le 10\%$ were considered acceptable and indicated that the standard errors were reasonably unbiased (Hoogland $\&$ Boomsma, 1998). Insufficient sample sizes might result in largely biased SEs due to increased uncertainty around the parameter estimates [@bollenTestingStructuralEquation1993; @byrneStructuralEquationModeling2016]. Given that extremely large SE values appeared in a small number of replications, a robust version of relative SE bias was also calculated:
\begin{equation}
Robust\ Relative\ SE\ Bias = \frac{MDN(\widehat{SE_{r}}) - MAD}{MAD},
\end{equation}
where $MDN$ represented the median value of the estimated SE values and $MAD$ was the empirical median-absolute-deviation of SE values. The MAD was defined by the median of absolute deviations from the median of sample, such that $MAD = b*MDN(|\widehat{SE}_{r} - MDN(SE)|)$ where $b$ is a scale factor set to 1.4826 to match the standard deviation of a normal distribution so that MAD can be a consistent estimator for $SD$ [@huberRobustStatistics2011; @rousseeuwAlternativesMedianAbsolute1993]. In the context of biased SEs, we did not assume a specific distribution of SEs (e.g., normal distribution) and hence we used the median due to its robustness to non-normal distributions with skewed data and outliers [@rousseeuwRobustStatisticsOutlier2011]. In addition, MAD measured variability around the median and could serve as a robust substitute to standard deviation that could be inflated by outliers or non-normality [@daszykowskiRobustStatisticsData2007]. 

### Outlier Proportion of SE
Extreme SE values might appear due to small samples and convergence issues, potentially leading to biased estimates of variance (Bentler & Yuan, 1999). To provide supplemental information on SE estimates, we included outlier detection using the interquartile range (IQR) method:
\begin{equation}
O_{a} \not\in (Q_{1} - 1.5 \times IQR, \ Q_{3} + 1.5 \times IQR),
\end{equation}
where $O_{a}$ was an observation of outlier for $a$ = 1, 2, ..., b. $IQR$ captured the spread of the middle 50$\%$ of the sample SEs by $IQR = Q_{3} \ - \ Q_{1}$, where $Q_{1}$ and $Q_{3}$ were the 25th percentile and the 75th percentile of the sample. The outlier proportion was then calculated by $b/R$ where $b$ represented the total count of identified outliers. Like the robust relative SE bias, the IQR method did not rely on the assumption of normal distribution, thus making it versatile across any distribution [@dekkingModernIntroductionProbability2005a].

### Root Mean Squre Error
Root mean square error quantified the average magnitude of the difference between the interaction estimates and the true value, reflecting both the bias and variability of the estimates across replications:
\begin{equation}
RMSE = \sqrt{R^{-1}\Sigma^{R}_{r = 1}(\hat{\gamma}_{xm_{r}} - \gamma_{xm})^2}.
\end{equation}

Methods with lower RMSE are on average more accurate in estimating $\hat{\gamma}_{xm}$ [@harwellStrategyUsingBias2019]. RMSE was most informative when comparing across methods under the same simulated conditions by isolating factors of sample size, model complexity, and the amount of disturbance.

### Empirical Type I Error Rate and Statistical Power

The empirical type I error informed the probability of incorrectly rejecting the null hypothesis that the latent interaction effect is not significant (i.e., $H_{0}: \gamma_{xm} = 0$) at the significance level ($\alpha = .05$). The type I error rate was computed across 2,000 replications by determining the proportion of instances where a Type I error occurred. An empirical Type I error rate within approximately 0.025 to 0.075 is often considered acceptable and the statistical tests are considered robust in the context of simulation studies (Bradley, 1978). Statistical power measures a method’s ability to detect an effect if one truly exists. In this study, it was defined as the proportion of correctly rejecting the null hypothesis when the interaction effect truly exists (i.e., $H_{a}: \gamma_{xm} = 0.3$). 

# Results
For all methods across all simulation conditions, the convergence rate was 100%, indicating that all the replications did not generate errors from the `lavaan` program. 

## Raw Bias and Standardized Bias for $\gamma_{xm}$

As outlined in Table 1, an examination of all simulation conditions revealed that the absolute values of SB associated with the estimate of $\gamma_{xm}$ using the latent interaction methods consistently remained within the predetermined acceptable threshold of .40 (SB = -.03 ~ .22). As for RB, the latent interaction methods as well showed generally smaller absolute values (RB = .00 ~ .10), particularly for RAPI and 2S-PA-Int (RB = .00 ~ .03). MMR was less comparable as it yielded extraordinarily larger RB and SB values especially under the condition of low item reliability, indicating its inability to handle measurement error. A discernible pattern in the impact of correlation between first-order latent predictors on $\gamma_{xm}$ was not identified. Regarding the influence of item reliability, all methods consistently yielded decreasing magnitude of SB and RB as $\rho$ increased from 0.7 to 0.9, implying that the estimation of $\gamma_{xm}$ gradually became more accurate as the amount of measurement error within first-order indicators reduced. 

The RB values generally became smaller as sample size increased for the latent interaction methods, which aligned with the statistical property of SEM models such that larger sample sizes tend to yield more accurate and reliable parameter estimates and reduced sampling errors. However, this pattern was not entirely consistent with the absolute SB values. As sample sizes increased, the empirical standard error of the estimates decreased, which could amplify the absolute SB. For instance, as the sample size increased while holding $\rho = .80$ and $Corr[\xi_{x}, \xi_{m}] = 0.3$, the magnitude of RB decreased from .02 to .01 for RAPI, while the absolute SB first increased from .05 to .13 and then decreased to .11. These findings suggested that the pattern observed in RB values might be influenced or masked by changes in the empirical standard error, highlighting the need for careful interpretation of SB values in standard units.

It was found that the absolute SB values for the latent interaction methods were almost positive, except for some conditions under high reliability and large sample size. The results were consistent with previous findings for RAPI and matched-pair UPI such that they tended to yield overestimated interaction estimates with high correlations between first-order latent predictors and low reliability (Marsh et al., 2004; Hsiao et al., 2018). 2S-PA-Int as well demonstrated overestimation of $\gamma_{xm}$ across conditions. The MMR method universally underestimated the interaction effect, nevertheless. 

Overall, the latent interaction methods yielded comparably low standardized biases across simulation conditions, which was acceptable for practical use. 

```{r standardized bias (raw bias), message=FALSE, warning=FALSE}
raw_bias <- sim_alt %>%
  dplyr::select(N:raw_bias.tspa_yint_est) %>%
  dplyr::select(-gamma_xm) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(raw_bias) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "RAPI", "Matched-Pair UPI", "2SPA")

raw_bias_wide <- raw_bias %>%
  pivot_wider(names_from = `$\\rho$`,
              values_from = c("MMR", "RAPI", "Matched-Pair UPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), round, 2))
names(raw_bias_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

std_bias <- sim_alt %>% 
  dplyr::select(N:rel, std_bias.reg_yint_est:std_bias.tspa_yint_est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) 
names(std_bias) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "RAPI", "Matched-Pair UPI", "2SPA")

std_bias_wide <- std_bias %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "RAPI", "Matched-Pair UPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`),
         across(where(is.numeric), round, 2))
names(std_bias_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

standardized_bias <- raw_bias_wide
for (col_idx in 3:ncol(raw_bias_wide)) {
  combined_values <- mapply(FUN = function(x, y) paste(y, "\ (", x, ")", sep = ""),
                            x = raw_bias_wide[, col_idx], y = std_bias_wide[, col_idx])
  standardized_bias[, col_idx] <- combined_values
}

standardized_bias_table <- apa_table(standardized_bias,
                                    escape = F,
                                    caption = "Standardized Bias (Raw Bias) for $\\gamma_{xm} (= 0.3)$ over 2,000 Replications.",
                                    align = c(rep("c", ncol(standardized_bias))),
                                    col_spanners = list(`MMR` = c(3, 5), `RAPI` = c(6, 8), 
                                                        `Matched-Pair UPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
                                    landscape = TRUE,
                                    font_size = "tiny",
                                    note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; MMR = moderated multiple regression method; RAPI = reliability-adjusted product indicator method; Matched-Pair UPI = matched-pair product unconstrained indicator method; 2S-PA-Int = two-stage path analysis with interaction method. Raw biases are shown in pararenthese. Note that numerical values have been rounded to two decimal places for consistency, which means that some values, while very close to 0 but not exactly 0, are displayed as 0.")

standardized_bias_table
```

## Relative SE Bias of $\gamma_{xm}$

Table 2 presents the robust relative standard error (SE) bias ratio along with the proportions of SE outliers when $\gamma_{xm} = 0.3$. Values outside the -10% to 10% range were bolded for emphasis. Generally, the values of robust relative SE bias for MMR went beyond the acceptable range for almost all conditions and indicated downward bias up to 17.3%. 

RAPI, matched-pair UPI, and 2S-PA-Int remained within the -10% to 10% range across conditions of medium to high reliability levels. However, matched-pair UPI produced two relative SE values outside the acceptable range under conditions of small sample size ($\textit{N} = 100$) and low reliability ($\rho = 0.70$): -10.58% and -13.68%, indicating that the SE values were negatively biased. Similarly, RAPI showed unacceptable relative SE biases under various conditions of low reliability ($\rho = 0.70$), implying that RAPI might introduce more uncertainty in the interaction effect estimation when measurement error was high.

The outlier proportions of SEs, identified using the IQR method, showed a declining trend for all methods as sample size increased and reliability levels improved, which indicated that the estimation of $\gamma_{xm}$ became more accurate and stable with fewer extreme SE values. It was found that the outlier proportions for MMR were apparently lower than the latent interaction methods across conditions, suggesting that MMR yielded fewer extreme values in SEs.

However, the robust relative SE bias did not demonstrate a clear pattern associated with $\rho$ (reliability) and $N$ (sample size). It was observed that even under conditions of large sample size and high reliability, the robust relative SE bias could be higher than those under less favorable conditions. Furthermore, the overall values of the relative SE bias were negative for matched-pair UPI and 2S-PA-Int, and predominantly positive for RAPI. This pattern suggested that the SEs were systematically underestimated for matched-pair UPI and 2S-PA-Int, while they were overestimated for RAPI.

```{r MAD relative SE bias with outliers proportion, message=FALSE, warning=FALSE}
MAD <- sim_alt %>% 
  dplyr::select(N:rel, stdMed_rse_bias.reg_yint_se:stdMed_rse_bias.tspa_yint_se) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(stdMed_rse_bias.reg_yint_se = stdMed_rse_bias.reg_yint_se*100,
         stdMed_rse_bias.rapi_yint_se = stdMed_rse_bias.rapi_yint_se*100,
         stdMed_rse_bias.upi_yint_se = stdMed_rse_bias.upi_yint_se*100,
         stdMed_rse_bias.tspa_yint_se = stdMed_rse_bias.tspa_yint_se*100)
names(MAD) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "RAPI", "Matched-Pair UPI", "2SPA")

MAD_wide <- MAD %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR","RAPI", "Matched-Pair UPI", "2SPA"), 
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(MAD_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

outlier_se <- sim_alt %>% 
  dplyr::select(N:rel, outlier_se.reg_yint_se:outlier_se.tspa_yint_se) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(across(where(is.numeric), ~sprintf("%.2f", .)))
  
names(outlier_se) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR","RAPI", "Matched-Pair UPI", "2SPA")

outlier_se_wide <- outlier_se %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "RAPI", "Matched-Pair UPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`)) 
names(outlier_se_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

MAD_bias <- MAD_wide
for (col_idx in 3:ncol(MAD_wide)) {
  combined_values <- mapply(FUN = function(x, y) paste(y, "\ (", x, ")", sep = ""),
                            x = outlier_se_wide[, col_idx], y = MAD_wide[, col_idx])
  MAD_bias[, col_idx] <- combined_values
}

bold_if_larger_than_10 <- function(cell) {
  first_number <- as.numeric(str_extract(cell, "^[^\\(]+"))
  if (!is.na(first_number) && abs(first_number) > 10) {
    return(sprintf("\\textbf{%s}", cell))
  } else {
    return(cell)
  }
}

for (i in 3:ncol(MAD_bias)) {
  # Applying the formatting function to each element of the column
  MAD_bias[[i]] <- sapply(MAD_bias[[i]], bold_if_larger_than_10)
}

MAD_bias_table <- apa_table(MAD_bias, 
            escape = F,
            caption = "Robust Relative Standard Error (SE) Bias Ratio (Outlier Proportion of SE; $\\%$) for $\\gamma_{xm} (= 0.3)$ over 2,000 Replications.",
            align = c(rep("c", ncol(MAD_bias))),
            col_spanners = list(`MMR` = c(3, 5), `RAPI` = c(6, 8),
                                `Matched-Pair UPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
            landscape = TRUE,
            font_size = "tiny",
            note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; MMR = moderated multiple regression method; RAPI = reliability-adjusted product indicator method; Matched-Pair UPI = matched-pair product unconstrained indicator method; 2S-PA-Int = two-stage path analysis with interaction method. Outlier proportions of SE are shown in parenthese and all the numbers were percentages. Note that relative SE bias values outside the acceptable range of [-10$\\%$, 10$\\%$] are bolded.")

MAD_bias_table
```

## Coverage Rate of 95% CI of $\gamma_{xm}$

As shown in Table 3, the coverage rates of the 95% confidence interval (CI) for MMR largely fell outside the acceptable range (91% to 98%), while the latent interaction methods performed much better. Specifically, RAPI and 2S-PA-Int yielded coverage rates well within the acceptable range across all conditions, with RAPI ranging from 95.20% to 97.50% and 2S-PA-Int from 93.40% to 95.55%. Two estimates from matched-pair UPI fell below 91%, under the conditions of small sample size ($\textit{N} = 100$) and low reliability ($\rho = 0.70$).

No clear trends in coverage rate were observed within the methods in relation to sample size, population reliability level, or the correlation between first-order latent variables. However, across the latent interaction methods, RAPI consistently demonstrated the highest coverage rate, followed by 2S-PA-Int, and UPI showing the lowest. This ranking suggested that RAPI had the highest likelihood of capturing the true interaction effect, with 2S-PA-Int and UPI following in performance, when the true interaction effect was present.

```{r coverage rate, message=FALSE, warning=FALSE}
coverage <- sim_alt %>% 
  dplyr::select(N:rel, coverage.reg_yint_est:coverage.tspa_yint_est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel) %>%
  mutate(coverage.reg_yint_est = coverage.reg_yint_est*100,
         coverage.rapi_yint_est = coverage.rapi_yint_est*100,
         coverage.upi_yint_est = coverage.upi_yint_est*100,
         coverage.tspa_yint_est = coverage.tspa_yint_est*100) 
names(coverage) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "RAPI", "Matched-Pair UPI", "2SPA")

coverage_wide <- coverage %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "RAPI", "Matched-Pair UPI", "2SPA"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(coverage_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

bold_if_less_than_91 <- function(cell) {
  if (!is.na(cell) && abs(cell) < 91) {
    return(sprintf("\\textbf{%s}", cell))
  } else {
    return(cell)
  }
}

for (i in 3:ncol(coverage_wide)) {
  # Applying the formatting function to each element of the column
  coverage_wide[[i]] <- sapply(coverage_wide[[i]], bold_if_less_than_91)
}

coverage_table <- apa_table(coverage_wide, 
            escape = F,
            caption = "Coverage Rate of 95 $\\%$ Confidence Interval (CI) for $\\gamma_{xm} (= 0.3)$ over 2,000 Replications.",
            align = c(rep("c", ncol(coverage))),
            col_spanners = list(`MMR` = c(3, 5), `RAPI` = c(6, 8),
                                `Matched-Pair UPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
            landscape = TRUE,
            font_size = "small",
            note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; MMR = moderated multiple regression method; RAPI = reliability-adjusted product indicator method; Matched-Pair UPI = matched-pair product unconstrained indicator method; 2S-PA-Int = two-stage path analysis with interaction method. Coverage rates not reaching the acceptable threshold of 91$\\%$ are bolded.")

coverage_table
```

## RMSE of $\gamma_{xm}$

Table 4 exhibited that the RMSE values for $\gamma_{xm}$ decreased as the sample size increased and the reliability level increased for all methods. The point estimates of $\gamma_{xm}$ for MMR showed the smallest RMSE across conditions, compared to the latent interaction methods. 2S-PA-Int, among the latent interaction methods, showed the least (or equally least) RMSE values across all conditions. For instance, under conditions of small sample size and low reliability, the RMSE values for 2S-PA-Int ranged from 0.20 to 0.32, while those for RAPI and matched-pair UPI ranged from 0.25 to 0.61 and 0.34 to 0.39, respectively. Notably, as reliability increased ($\rho = 0.90$), the discrepancies in RMSE values across methods became less pronounced, indicating that all methods tended to produce more accurate and stable estimates of the interaction effect under conditions of high reliability.

```{r rmse, message=FALSE, warning=FALSE}
rmse <- sim_alt %>% 
  dplyr::select(N:rel, rmse.reg_yint_est:rmse.tspa_yint_est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(rmse) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "RAPI", "Matched-Pair UPI", "2S-PA-Int")

rmse_wide <- rmse %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "RAPI", "Matched-Pair UPI", "2S-PA-Int"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(rmse_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

rmse_table <- apa_table(rmse_wide, 
            escape = F,
            caption = "Root Mean Square Error (RMSE) for $\\gamma_{xm} (= 0.3)$ over 2,000 Replications.",
            align = c(rep("c", ncol(rmse))),
            col_spanners = list(`MMR` = c(3, 5), `RAPI` = c(6, 8),
                                `Matched-Pair UPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
            landscape = TRUE,
            font_size = "small",
            note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; MMR = moderated multiple regression method; RAPI = reliability-adjusted product indicator method; Matched-Pair UPI = matched-pair product unconstrained indicator method; 2S-PA-Int = two-stage path analysis with interaction method. Note that the methods are grouped in the second-order header for comparing RMSE under the same conditions.")

rmse_table
```

## Empirical Type I Error Rate and Statistical Power

For conditions where $\gamma_{xm} = 0$, the Type I error rates, calculated as the frequency with which the null hypothesis (i.e., $\gamma_{xm} = 0$) was falsely rejected, ranged from 0.01 to 0.08 across all methods. While the differences across methods were not particularly remarkable, MMR yielded Type I error rates that exceeded the critical value of 0.05. In contrast, the latent interaction methods did not show any Type I error rates above the critical value. RAPI and matched-pair UPI achieved the lowest error rates under conditions of small sample size and low-to-medium reliability, while 2S-PA-Int produced error rates that were very close to the critical value.

Regarding statistical power, MMR exhibited higher power than the latent interaction methods when the sample size was small to medium, although this advantage diminished as the sample size increased. Among the latent interaction methods, 2S-PA-Int demonstrated noticeably higher power under conditions of small sample size and low reliability. With a large sample size, all methods performed very well, and the differences in power between them became negligible.

```{r type I error, message=FALSE, warning=FALSE}
type1 <- sim_null %>% 
  dplyr::select(N:rel, type1.reg_yint_est:type1.tspa_yint_est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(type1) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "RAPI", "Matched-Pair UPI", "2S-PA-Int")

type1_wide <- type1 %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "RAPI", "Matched-Pair UPI", "2S-PA-Int"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(type1_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

type1_table <- apa_table(type1_wide, 
            escape = F,
            caption = "Empirical Type I Error Rate for $\\gamma_{xm} (= 0)$ over 2,000 Replications.",
            align = c(rep("c", ncol(rmse))),
            col_spanners = list(`MMR` = c(3, 5), `RAPI` = c(6, 8),
                                `Matched-Pair UPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
            landscape = TRUE,
            font_size = "footnotesize",
            note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; MMR = moderated multiple regression method; RAPI = reliability-adjusted product indicator method; Matched-Pair UPI = matched-pair product unconstrained indicator method; 2S-PA-Int = two-stage path analysis with interaction method. Note that the methods are grouped in the second-order header for comparing the empirical Type I error rate under the same conditions.")

type1_table
```

```{r power, message=FALSE, warning=FALSE}
power <- sim_alt %>% 
  dplyr::select(N:rel, power.reg_yint_est:power.tspa_yint_est) %>%
  arrange(N) %>%
  relocate(cor_xm, .after = rel)
names(power) <- c("$\\textit{N}$", "$\\rho$", "$Corr(\\xi_{x}, \\xi_{m})$", "MMR", "RAPI", "Matched-Pair UPI", "2S-PA-Int")

power_wide <- power %>%
  pivot_wider(names_from = `$\\rho$`,  
              values_from = c("MMR", "RAPI", "Matched-Pair UPI", "2S-PA-Int"), #
              names_prefix = "corr_") %>%
  mutate(`$\\textit{N}$` = ifelse(`$Corr(\\xi_{x}, \\xi_{m})$` != 0, " ", `$\\textit{N}$`))
names(power_wide) <- c("$\\textit{N}$", "$Corr(\\xi_{x}, \\xi_{m})$", paste(rep(c("$\\rho = .70$", "$\\rho = .80$", "$\\rho = .90$"), 4)))

power_table <- apa_table(power_wide, 
            escape = F,
            caption = "Statistical Power for $\\gamma_{xm} (= 0.3)$ over 2,000 Replications.",
            align = c(rep("c", ncol(rmse))),
            col_spanners = list(`MMR` = c(3, 5), `RAPI` = c(6, 8),
                                `Matched-Pair UPI` = c(9, 11), `2S-PA-Int` = c(12, 14)),
            landscape = TRUE,
            font_size = "footnotesize",
            note = "$\\textit{N}$ = sample size; $Corr(\\xi_{x}, \\xi_{m})$ = correlation between $\\xi_{x}$ and $\\xi_{m}$; $\\rho$ = reliability level; MMR = moderated multiple regression method; RAPI = reliability-adjusted product indicator method; Matched-Pair UPI = matched-pair product unconstrained indicator method; 2S-PA-Int = two-stage path analysis with interaction method. Note that the methods are grouped in the second-order header for comparing statistical power under the same conditions.")

power_table
```

# Empirical Demonstration Using Real Data

In this section, we demonstrated the application and comparison of the three latent interaction methods discussed in the previous sections by replicating the findings of an interaction model from Park's (2011) study. Park (2011) investigated the interaction effect between intrinsic motivation (IM) and extrinsic motivation (EM) on reading performance using hierarchical linear modeling. The study revealed a significant interaction effect, suggesting that the impact of IM on participants' reading scores was dependent on the level of EM. In Park's original analysis, factor scores for the two motivation constructs were computed and used as explanatory variables. However, the study did not examine the interaction effects at the level of individual motivation items. Therefore, in this demonstration, we focused on observed items by applying latent interaction methods to reanalyze the data, and further compared our results with those of the original study.

Data for this demonstration were drawn from the Progress in International Reading Literacy Study (PIRLS) 2006, an international assessment of reading literacy among fourth-grade students (Mullis et al., 2007). Consistent with Park's (2011) original study, only the U.S. sample was considered, which represented fourth-grade students from across all 50 states and the District of Columbia. The initial sample size consisted of 5,190 participants. After excluding those with missing responses on any of the observed motivation items, the final sample included 4,900 students. Since multilevel and multi-group analyses were not the focus of this demonstration, we conducted and reported only student-level analyses.

The model included six observed items, with three items measuring IM (e.g., "I read only if I have to," "I think reading is boring," "I enjoy reading") and three items measuring EM (e.g., "I like talking about books with other people," "I would be happy if someone gave me a book as a present," "I need to read well for my future"). All items were scored on a four-point Likert scale, ranging from 0 ("disagree a lot") to 3 ("agree a lot"). To avoid computational confusion and ensure consistency in interpretation, four items were recoded so that all items were positively oriented, meaning higher scores indicated higher levels of reading motivation.

To replicate the findings of Park (2011), we hypothesized that EM would be negatively related to students' reading performance, IM would be positively related to performance, and there would be a significant interaction between the two types of reading motivation on performance. The demonstration of the three latent interaction methods was conducted in R, and the methods were compared based on the point estimates of path coefficients, along with their standard errors and significance levels. The PIRLS 2006 data utilized five plausible values to accurately assess students' reading performance, addressing the substantial uncertainty in estimating individual characteristics (Martin et al., 2007). Following the guidelines provided in the PIRLS 2006 technical report, we fitted the model on each plausible value as the dependent variable for each latent interaction method and then combined the estimates using Rubin's rules.^[A detailed treatment of the use of plausible values can be found in PIRLS 2006 Technical Report [@martinProgressInternationalReading2007]]

A two-factor measurement model was fitted to assess the fit of the motivation constructs. The fit indices indicated an acceptable fit to the data: $\chi^2(df) = 250.382$ with $\textit{df} = 8$, $RMSEA = .08$, $CFI = .96$, $TLI = .93$, and $SRMR = .04$. Although the significant $\chi^2$ value suggested a significant discrepancy between the observed and model-implied covariance matrices, the sensitivity of $\chi^2$ to large sample sizes often results in significant values even with trivial discrepancies between the matrices. Therefore, greater emphasis should be placed on comparative indices [@huCutoffCriteriaFit1999]. Regarding the other fit indices, the $CFI$ indicated a good fit ($> .95$), and the $TLI$ indicated an acceptable fit ($> .90$). The $RMSEA$ and $SRMR$ values did not exceed the acceptable thresholds of .08 and .05, respectively [@browneAlternativeWaysAssessing1992; @joreskogLISRELStructuralEquation1993]. Overall, the measurement model demonstrated an adequate fit to the data.

Table 7 listed the point estimates of path coefficient, standard error of measurement, and p-value across the three methods, even though first-order effects (i.e., $\hat{\beta}_{IM}$ and $\hat{\beta}_{EM}$) were not the primary focus of this research. It was observed that the first-order of IM and EM, and the interaction effect between them were significant across all three methods (i.e., $\textit{p} < .001$). Consistent with the hypotheses and the results of Park (2011), we found that higher levels of IM were positively correlated with higher reading performance scores (all $\textit{p}$ values $< .001$), whereas higher levels of EM were negatively associated with performance (all $\textit{p}$ values $< .001$) across methods. As for the interaction effect, we found significantly positive correlation between the latent interaction term and performance score (all $\textit{p}$ values $< .001$). Regardless of signs, we could observe that matched-pair UPI, RAPI, and 2S-PA-Int yield comparable magnitude of parameter estimates for first-order and interaction effects (i.e., 51.10 to 60.39 for $\hat{\beta}_{IM}$; 21.86 to 29.26 for $\hat{\beta}_{EM}$; 10.78 to 11.12 for $\hat{\beta}_{IM \times EM}$). RAPI consistently yielded higher estimates then matched-pair UPI and 2S-PA-Int. While the SE estimates were also similar across approaches in terms of magnitude, RAPI notably yielded slightly higher SE estimates than the other two approaches.

```{r PIRL2006 Example, message=FALSE, warning=FALSE, include=FALSE}
PIRLS2006 <- read_sav(here("Paper", "PIRLS2006", "asgusar2.sav"))

PIRLS_Data <- PIRLS2006 %>%
  select(ASBGRST1:ASBGRST6, ASRREA01:ASRREA05) %>%
  drop_na()

PIRLS_Data <- PIRLS_Data %>%
  mutate(across(
    .cols = c(ASBGRST2, ASBGRST3, ASBGRST5, ASBGRST6), # replace with your actual column names
    .fns = ~ recode(as.numeric(.x), `1` = 4, `2` = 3, `3` = 2, `4` = 1),
    .names = "{.col}_recode"
  )) %>%
  drop_na()

# Measurement Model
IM <- PIRLS_Data %>% select(ASBGRST1, ASBGRST4, ASBGRST6_recode)
EM <- PIRLS_Data %>% select(ASBGRST2_recode, ASBGRST3_recode, ASBGRST5_recode)

model <- "IM =~ ASBGRST1 + ASBGRST4 + ASBGRST6_recode
          EM =~ ASBGRST2_recode + ASBGRST3_recode + ASBGRST5_recode"

fitmeasures(sem(model, PIRLS_Data))

# Dependent Vars
DVs <- c("ASRREA01", "ASRREA02", "ASRREA03", "ASRREA04", "ASRREA05")

# UPI
mod_upi <- "
          IM =~ ASBGRST1 + ASBGRST4 + ASBGRST6_recode
          EM =~ ASBGRST2_recode + ASBGRST3_recode + ASBGRST5_recode

          DV ~ b1*IM + b2*EM + b3*IM:EM

          IM ~~ v1*IM
          EM ~~ v2*EM
          beta1 := b1*sqrt(v1)
          beta2 := b2*sqrt(v2)
          beta3 := b3*sqrt(v1)*sqrt(v2)
          "
# Fit the model using upi()
beta1_upi <- c()
beta2_upi <- c()
beta3_upi <- c()
se1_upi <- c()
se2_upi <- c()
se3_upi <- c()
p1_upi <- c()
p2_upi <- c()
p3_upi <- c()

for (dv in DVs) {
  mod_upi_temp <- gsub("DV", dv, mod_upi)  # Replace "DV" with the current dv in the loop
  fit_upi <- upi(mod_upi_temp, PIRLS_Data, mode = "match")
  beta1_upi <- unname(c(beta1_upi, coef(fit_upi, type = "user")["beta1"]))
  beta2_upi <- unname(c(beta2_upi, coef(fit_upi, type = "user")["beta2"]))
  beta3_upi <- unname(c(beta3_upi, coef(fit_upi, type = "user")["beta3"]))
  se1_upi <- c(se1_upi, sqrt(vcov(fit_upi, type = "user")["beta1", "beta1"]))
  se2_upi <- c(se2_upi, sqrt(vcov(fit_upi, type = "user")["beta2", "beta2"]))
  se3_upi <- c(se3_upi, sqrt(vcov(fit_upi, type = "user")["beta3", "beta3"]))
  par_upi <- parameterEstimates(fit_upi)
  p1_upi <- c(p1_upi, as.integer(par_upi %>% filter(label == "beta1") %>% select(pvalue)))
  p2_upi <- c(p2_upi, as.integer(par_upi %>% filter(label == "beta2") %>% select(pvalue)))
  p3_upi <- c(p3_upi, as.integer(par_upi %>% filter(label == "beta3") %>% select(pvalue)))
}

beta1_upi_avg <- mean(beta1_upi)
beta2_upi_avg <- mean(beta2_upi)
beta3_upi_avg <- mean(beta3_upi)

# Merge SE
combine_se <- function(combined_path, se_val) {
  within_var <- mean(se_val^2)
  between_var <- var(combined_path)
  m <- length(combined_path)
  total_variance <- within_var + (1 + 1/m) * between_var
  return(sqrt(total_variance))
}

se1_upi_avg <- combine_se(beta1_upi, se1_upi)
se2_upi_avg <- combine_se(beta2_upi, se2_upi)
se3_upi_avg <- combine_se(beta3_upi, se3_upi)

# Merge P-values
combine_p_value <- function(avg_path, combined_se) {
  combined_z <- avg_path / combined_se
  combined_p_value <- 2 * (1 - pnorm(abs(combined_z)))
  return(combined_p_value)
}

p1_upi_avg <- combine_p_value(beta1_upi_avg, se1_upi_avg)
p2_upi_avg <- combine_p_value(beta2_upi_avg, se2_upi_avg)
p3_upi_avg <- combine_p_value(beta3_upi_avg, se3_upi_avg)

# RAPI
mod_rapi <- "
          IM =~ ASBGRST1 + ASBGRST4 + ASBGRST6_recode
          EM =~ ASBGRST2_recode + ASBGRST3_recode + ASBGRST5_recode

          DV ~ b1*IM + b2*EM + b3*IM:EM

          IM ~~ v1*IM
          EM ~~ v2*EM
          beta1 := b1*sqrt(v1)
          beta2 := b2*sqrt(v2)
          beta3 := b3*sqrt(v1)*sqrt(v2)
          "
# Fit the model using upi()
beta1_rapi <- c()
beta2_rapi <- c()
beta3_rapi <- c()
se1_rapi <- c()
se2_rapi <- c()
se3_rapi <- c()
p1_rapi <- c()
p2_rapi <- c()
p3_rapi <- c()

for (dv in DVs) {
  mod_rapi_temp <- gsub("DV", dv, mod_rapi)  # Replace "DV" with the current dv in the loop
  fit_rapi <- rapi(mod_rapi_temp, PIRLS_Data)
  beta1_rapi <- c(beta1_rapi, coef(fit_rapi, type = "user")["beta1"])
  beta2_rapi <- c(beta2_rapi, coef(fit_rapi, type = "user")["beta2"])
  beta3_rapi <- c(beta3_rapi, coef(fit_rapi, type = "user")["beta3"])
  se1_rapi <- c(se1_rapi, sqrt(vcov(fit_rapi, type = "user")["beta1", "beta1"]))
  se2_rapi <- c(se2_rapi, sqrt(vcov(fit_rapi, type = "user")["beta2", "beta2"]))
  se3_rapi <- c(se3_rapi, sqrt(vcov(fit_rapi, type = "user")["beta3", "beta3"]))
  par_rapi <- parameterEstimates(fit_rapi)
  p1_rapi <- c(p1_rapi, as.integer(par_rapi %>% filter(label == "beta1") %>% select(pvalue)))
  p2_rapi <- c(p2_rapi, as.integer(par_rapi %>% filter(label == "beta2") %>% select(pvalue)))
  p3_rapi <- c(p3_rapi, as.integer(par_rapi %>% filter(label == "beta3") %>% select(pvalue)))
}

beta1_rapi_avg <- mean(beta1_rapi)
beta2_rapi_avg <- mean(beta2_rapi)
beta3_rapi_avg <- mean(beta3_rapi)

se1_rapi_avg <- combine_se(beta1_rapi, se1_rapi)
se2_rapi_avg <- combine_se(beta2_rapi, se2_rapi)
se3_rapi_avg <- combine_se(beta3_rapi, se3_rapi)

p1_rapi_avg <- combine_p_value(beta1_rapi_avg, se1_rapi_avg)
p2_rapi_avg <- combine_p_value(beta2_rapi_avg, se2_rapi_avg)
p3_rapi_avg <- combine_p_value(beta3_rapi_avg, se3_rapi_avg)

# 2SPA
mod_cfa <- '
IM =~ ASBGRST1 + ASBGRST4 + ASBGRST6_recode
EM =~ ASBGRST2_recode + ASBGRST3_recode + ASBGRST5_recode
'
# Obtain factor scores
fs_dat <- get_fs(PIRLS_Data, model = mod_cfa, method = "Bartlett", std.lv = TRUE)
# Obtain factor product
fs_dat$fs_int <- fs_dat$fs_IM * fs_dat$fs_EM
# Centering the product variable
fs_dat$fs_int <- fs_dat$fs_int - mean(fs_dat$fs_int)
fs_dat$fs_int_se <- sqrt(1 * fs_dat$fs_IM_se^2 + 1 * fs_dat$fs_EM_se^2 + fs_dat$fs_IM_se^2*fs_dat$fs_EM_se^2)

beta1_tspa <- c()
beta2_tspa <- c()
beta3_tspa <- c()
se1_tspa <- c()
se2_tspa <- c()
se3_tspa <- c()
p1_tspa <- c()
p2_tspa <- c()
p3_tspa <- c()

for (dv in DVs) {
  if ("ReadingScore" %in% names(fs_dat)) {
    fs_dat$ReadingScore <- NULL
  }
  fs_dat$ReadingScore <- PIRLS_Data[[dv]]
  fit_tspa <- tspa(model = "ReadingScore ~ b1*IM + b2*EM + b3*IM:EM
                          beta1 := b1 * sqrt(v1)
                          beta2 := b2 * sqrt(v2)
                          beta3 := b3 * sqrt(v1) * sqrt(v2)",
                   data = fs_dat,
                   se = list(IM = fs_dat$fs_IM_se[1],
                             EM = fs_dat$fs_EM_se[1]))
  beta1_tspa <- c(beta1_tspa, coef(fit_tspa, type = "user")["beta1"])
  beta2_tspa <- c(beta2_tspa, coef(fit_tspa, type = "user")["beta2"])
  beta3_tspa <- c(beta3_tspa, coef(fit_tspa, type = "user")["beta3"])
  se1_tspa <- c(se1_tspa, sqrt(vcov(fit_tspa, type = "user")["beta1", "beta1"]))
  se2_tspa <- c(se2_tspa, sqrt(vcov(fit_tspa, type = "user")["beta2", "beta2"]))
  se3_tspa <- c(se3_tspa, sqrt(vcov(fit_tspa, type = "user")["beta3", "beta3"]))
  par_tspa <- parameterEstimates(fit_tspa)
  p1_tspa <- c(p1_tspa, as.integer(par_tspa %>% filter(label == "beta1") %>% select(pvalue)))
  p2_tspa <- c(p2_tspa, as.integer(par_tspa %>% filter(label == "beta2") %>% select(pvalue)))
  p3_tspa <- c(p3_tspa, as.integer(par_tspa %>% filter(label == "beta3") %>% select(pvalue)))
}

beta1_tspa_avg <- mean(beta1_tspa)
beta2_tspa_avg <- mean(beta2_tspa)
beta3_tspa_avg <- mean(beta3_tspa)

se1_tspa_avg <- combine_se(beta1_tspa, se1_tspa)
se2_tspa_avg <- combine_se(beta2_tspa, se2_tspa)
se3_tspa_avg <- combine_se(beta3_tspa, se3_tspa)

p1_tspa_avg <- combine_p_value(beta1_tspa_avg, se1_tspa_avg)
p2_tspa_avg <- combine_p_value(beta2_tspa_avg, se2_tspa_avg)
p3_tspa_avg <- combine_p_value(beta3_tspa_avg, se3_tspa_avg)
```

```{r PIRL 2006 Table, echo=FALSE, message=FALSE, warning=FALSE}
# Create the data frame with LaTeX \makebox for width control
PIRL_result <- data.frame(
  Approach = c("\\makebox[4cm][c]{Matched-Pair UPI}", 
               "\\makebox[4cm][c]{RAPI}", 
               "\\makebox[4cm][c]{2S-PA-Int}"),
  Beta_1 = round(c(beta1_upi_avg, beta1_rapi_avg, beta1_tspa_avg), 3),
  SE_1 = round(c(se1_upi_avg, se1_rapi_avg, se1_tspa_avg), 3),
  p_1 = rep("\\makebox[2cm][c]{$< .001^{***}$}", 3),  
  Beta_2 = round(c(beta2_upi_avg, beta2_rapi_avg, beta2_tspa_avg), 3),
  SE_2 = round(c(se2_upi_avg, se2_rapi_avg, se2_tspa_avg), 3),
  p_2 = rep("\\makebox[2cm][c]{$< .001^{***}$}", 3), 
  Beta_3 = round(c(beta3_upi_avg, beta3_rapi_avg, beta3_tspa_avg), 3),
  SE_3 = round(c(se3_upi_avg, se3_rapi_avg, se3_tspa_avg), 3),
  p_3 = rep("\\makebox[2cm][c]{$< .001^{***}$}", 3)  
)

# Name the columns with LaTeX formatting
names(PIRL_result) <- c("Approach", 
                        "$\\hat{\\beta}_{IM}$", "$SE_{IM}$", "$\\textit{p}_{IM}$", 
                        "$\\hat{\\beta}_{EM}$", "$SE_{EM}$", "$\\textit{p}_{EM}$",
                        "$\\hat{\\beta}_{IM \\times EM}$", "$SE_{IM \\times EM}$", "$\\textit{p}_{IM \\times EM}$")

# Create the APA-style table
PIRL_table <- apa_table(
  PIRL_result,
  escape = FALSE,
  landscape = TRUE,
  caption = "Parameter Estimates of the Latent Interaction Effect with Three Methods.",
  align = rep("c", ncol(PIRL_result)),  # Center-align all columns
  note = "$\\textit{N}$ = 4,900. Matched-Pair UPI = matched-pair product unconstrained indicator method; RAPI = reliability-adjusted product indicator method; 2S-PA-Int = two-stage path analysis with interaction method. $\\hat{\\beta}_{IM}$ and $\\hat{\\beta}_{EM}$ denoted the first-order effect of IM and EM on reading performance scores, and $\\hat{\\beta}_{IM \\times EM}$ was their latent interaction effect. SE represented the standard error of measurement and $\\textit{p}$ denoted the significance value. The results showed significant first-order and latent interaction effects using all three methods.")

PIRL_table
```

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{/Users/jimmy_z/R Projects/2SPA-Int/Paper/Qual 1 Paper Draft/PIRL_Plot.png}
\caption{Structural Model of Illustrative Example from Park (2011).}
\caption*{\textit{Note. The model includes two first-order latent variables, intrinsic motivation and extrinsic motivation, depicted as ellipses. Their latent interaction term was depicted as a filled black circle. The dependent variable was observed and rendered as a rectangle.}}
\end{figure}

# Discussion

Applied researchers often explore complex relationships between variables, such as interactions. However, classical regression models, which assume that variables are free from measurement error, have been shown to yield biased estimates. As a result, latent variable approaches within the SEM framework are gaining prominence. In this study, we reviewed and compared the performance of three latent interaction methods, matched-pair UPI, RAPI, and 2S-PA-Int, in estimating interaction effects on congeneric items with varying factor loadings and measurement errors. Additionally, the regression-based approach using observed indicators, MMR, was included as a reference method.

We extended the 2S-PA model by @laiTwostagePathAnalysis2022a to support latent interaction estimation, naming it 2S-PA-Int. The primary distinction between matched-pair UPI, RAPI, and 2S-PA-Int lies in the formation of the latent interaction term. Matched-pair UPI constructs the latent interaction term using multiple product indicators (PIs) generated from first-order indicators, making it a multiple-indicator method. In contrast, RAPI and 2S-PA-Int use composite scores and factor scores as single indicators (SIs) for the latent interaction term, respectively.

Our results demonstrated that the MMR approach, based on observed indicators, consistently yielded substantially negative estimates of interaction effect path coefficients across multiple conditions. This finding can be attributed to the method's failure to properly account for measurement errors in the observed composites. These underestimated coefficients are consistent with previous research, which has emphasized the bias introduced by imperfect reliability when assessing interaction effects (Dunlap & Kemery, 1988; Evans, 1985). 

On the other hand, the latent interaction methods were capable of producing unbiased estimates of interaction effects by accounting for measurement errors, with the standardized bias (SB) and raw bias (RB) values consistently falling below the 0.40 threshold. However, RAPI and UPI exhibited notably positive SB values, indicating a tendency to overestimate interaction effects when true effects were present. These results are consistent with previous findings by @marshStructuralEquationModels2004a (using congeneric items with varying factor loadings), @hsiaoEvaluationTwoMethods2018a (using tau-equivalent items with varied error variances), and @hsiaoModelingMeasurementErrors2021 (using congeneric items), where matched-pair UPI and RAPI slightly overestimated interaction coefficients, albeit within acceptable limits. 2S-PA-Int also slightly tend to overestimate the path coefficient of interaction effects when they truly exist, indicating that latent interaction methods should be used with caution, particularly when researchers seek more conservative estimates of interaction effects.

With respect to coverage rates, RAPI demonstrated notably high coverage rates, ranging from 95% to 97% within the 95% confidence interval (CI), indicating greater precision in capturing true interaction effects compared to matched-pair UPI and 2S-PA-Int. While slightly lower than those of RAPI, 2S-PA-Int also achieved acceptable coverage rates (93% ~ 96%), suggesting its capacity for accurately capturing true interaction effects. In contrast, matched-pair UPI was more susceptible to small sample sizes and low reliability levels, underscoring its reduced robustness in such conditions. These results imply that both RAPI and 2S-PA-Int possess sufficient capability of effectively detecting interaction effects across varied conditions. However, matched-pair UPI's performance was less consistent, particularly when error variances differed among first-order indicators. This observation is in line with previous studies, though it should be noted that Marsh et al. (2004) did not evaluate matched-pair UPI with fully congeneric items, which may partly explain its reduced ability to capture true effects under such conditions. 

MMR failed to show sufficient coverage rates across almost all conditions, indicating that this approach struggles to capture the true interaction effects within the specified confidence intervals. By ignoring measurement error, MMR produces narrower confidence intervals and underestimates the variability in the interaction effect, which results in increasing the likelihood of missing the true effects. Although latent interaction methods can correct for bias in SE estimation, Hsiao and Lai (2018) noted that constraining measurement errors for highly reliable variables may lead to over-adjustment, particularly in small sample sizes. Our RMSE results supported this concern, as latent interaction methods generally exhibited higher RMSE values compared to MMR when the sample size was 100. However, among the latent interaction methods, 2S-PA-Int produced the lowest RMSE values, which were nearly comparable to those of MMR. Overall, latent interaction methods for composite scores are preferable to MMR when considering both precision and bias in the estimation of interaction effect, with 2S-PA-Int demonstrating the greatest potential among the methods.

In our analysis of latent interaction methods, sample size and reliability level significantly influenced the estimation of non-zero interaction effects. Smaller sample sizes and higher measurement error, indicated by lower reliability, resulted in larger absolute values of SB and RB. However, as sample size increased and error decreased, both SB and RB improved across all methods, suggesting better performance with larger samples. Higher reliability levels also led to more unbiased estimates across conditions. Relative SE bias followed a similar trend, with smaller biases observed for larger sample sizes and higher reliability. As reported in the previous study (Hsiao & Lai, 2018), RAPI generally exhibited greater relative SE bias than matched-pair UPI and 2S-PA-Int, especially under small sample sizes and low reliability, making it more prone to unstable interaction estimates. In terms of RMSE, both sample size and reliability had a clear effect, with larger sample sizes and higher reliability reducing RMSE values and improving accuracy. Among the methods, 2S-PA-Int consistently showed lower RMSE values and greater stability, particularly in challenging conditions with small sample sizes and low reliability, highlighting its robustness in estimating latent interaction effects.

Revisiting Marsh's criteria for an effective latent interaction model, 2S-PA-Int stands out for its simplicity as a single-indicator method and its efficient use of information through factor scores based on all first-order indicators. Models burdened with excessive indicators often face convergence issues due to complex covariance structures, potentially resulting in non-identifiable models [@bollenStructuralEquationsLatent1989d]. Moreover, @byrneStructuralEquationModeling2016 points out that too many indicators can introduce redundancy, unnecessarily complicating the model and increasing the risk of estimation problems. Therefore, 2S-PA-Int emerges as a better alternative to matched-pair UPI, especially in cases of small sample size and low reliability. Compared to RAPI, 2S-PA-Int also offers greater stability and accuracy in estimating interaction effects.

# Limitations and Future Directions

Marsh et al. (2004) demonstrated that all-pair UPI was not preferred due to its lack of substantial improvement in latent interaction estimation, despite having a slightly more complex model compared to matched-pair UPI. In our preliminary study, we investigated the performance of all-pair UPI on congeneric items, as Marsh et al. (2004) only examined parallel items without varying factor loadings and error variances. We found that while SB of interaction estimates produced by all-pair UPI were negligible for parallel items, they were more remarkable for congeneric factor items (with varied factor loadings but consistent error variances) and fully congeneric items. A likely explanation for the increased standardized biases is that, although RB values were very small across all item types, the corresponding standard errors systematically decreased as sample size increased. These results suggest that all-pair UPI did not perform as well as matched-pair UPI, which is why it was not included in our main study (Results of the preliminary simulation study of all-pair UPI are available at: [Supplemental Material](https://github.com/Gengrui-Zhang/2S-PA-Int/blob/main/Qual_1_Supplemental_Material/Supplemental-Material.pdf)).

As a limitation of our study, we did not evaluate or compare alternative methods using distribution analytic approaches, such as the latent moderated structural equation (LMS; Klein & Moosbrugger, 2000) method, since the focus was on product indicator methods. Previous research has shown that LMS tends to produce unbiased estimates of latent interaction effects with acceptable statistical power when applied to congeneric items with normal distributions (Hsiao et al., 2021; Cham et al., 2012). In future investigations, we plan to incorporate widely used alternative methods for comparison with the 2S-PA-Int approach. 

Previous studies on latent interaction effects have generally employed simple designs with two latent predictors and one interaction term, which may not fully capture the complexity of real-world scenarios involving multiple interaction terms. Furthermore, multilevel designs are increasingly common in educational, counseling, and organizational research (e.g., students nested in classrooms, patients nested in clinics, employees nested in companies). It would be valuable to explore the potential of 2S-PA-Int in handling more complex data structures, varying sample sizes, and reliability levels in these multilevel contexts.

Another limitation, as noted by Hsiao et al. (2018), is that RAPI may be more accessible when researchers are working with secondary data, where only composite scores and their reliability indices (e.g., Cronbach’s $\alpha$) are typically available. In cases where factor scores and their standard errors are not reported, it may not be possible to compute factor scores, thus limiting the applicability of 2S-PA-Int. Additionally, the congeneric items in our study were continuous with normal distributions. Since categorical data are often used in psychological research to capture qualitative aspects of human behavior, attitudes, and characteristics [@brownConfirmatoryFactorAnalysis2015; @klinePrinciplesPracticeStructural2016], 2S-PA-Int has yet to be evaluated with categorical items. However, given its ability to incorporate observation-specific standard error of measurement for each observation, we expect 2S-PA-Int to perform well in estimating latent interaction effects with categorical data in future studies.

\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
